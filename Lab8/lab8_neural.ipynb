{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e104bc-6c64-424f-bdf2-45a5df0f3581",
   "metadata": {},
   "source": [
    "# NLP Lab8 - Neural search for question answering\n",
    "\n",
    "**Author: Bartłomiej Jamiołkowski**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf9627-e483-4bc9-a1ba-f80cf9d4a417",
   "metadata": {},
   "source": [
    "The exercise introduces the problem of passage retrieval, an important step in factual question answering. \r\n",
    "This part concentrates on the methods for retrieving\r\n",
    "the content of documents that might be useful for answering the question. We compare lexical text\r\n",
    "representations (e.g. ElasticSearch default behaviour), with dense text representations (e.g. [multilingual E5](https://huggingface.co/intfloat/multilingual-e5-base) neural mode).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74292b40-9f72-444d-9a8f-1f5a8c3c1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "from transformers import AutoTokenizer, Trainer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datasets\n",
    "datasets.disable_progress_bar()\n",
    "datasets.logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19996b-02d8-43e0-91fe-24e456769f9e",
   "metadata": {},
   "source": [
    "## Tasks 1 -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f001d6-ea5e-49dd-834a-4e793458acc0",
   "metadata": {},
   "source": [
    "Read the documentation of the [document store](https://docs.haystack.deepset.ai/docs/document_store) and\r\n",
    "   the [retriever](https://docs.haystack.deepset.ai/docs/retriever) in the \r\n",
    "   [Haystack framework](https://haystack.deepset.ai/\n",
    "\n",
    "Install Haystack framework (e.g. with `pip install 'farm-haystack[all]'`).)."
   ]
  },
  {
   "attachments": {
    "3d8b299b-a6d7-4604-97fb-7bc115d76e82.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a573d89d-94d1-4c2b-8179-f11bd0e3d15e",
   "metadata": {},
   "source": [
    "![image.png](attachment:3d8b299b-a6d7-4604-97fb-7bc115d76e82.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039317e-1e7d-446f-8eeb-7b6dcf081c03",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293606c-6aeb-4b1b-8e7f-0bef4d8f7c37",
   "metadata": {},
   "source": [
    "Configure a document store based on Faiss supported by multilingual E5 model:\r\n",
    "   1. For Faiss use [multilingual E5](https://huggingface.co/intfloat/multilingual-e5-base) or [silver retriever base](https://huggingface.co/ipipan/silver-retriever-base-v1) encoder.\r\n",
    "   3. **Warning:** If you use E5, make sure to [properly configure](https://github.com/deepset-ai/haystack/issues/5242) the store.\r\n",
    "   4. In the case you have problems using Faiss, you can use `InMemoryDocumentStore`, but this will require to re-index\r\n",
    "      all documents each time the script is run, which is time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177fa5fc-8d0f-4aa5-9265-adfcee352caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever_scheme(document_store, model, k):\n",
    "    retriever = EmbeddingRetriever(\n",
    "        document_store = document_store,\n",
    "        embedding_model = model,\n",
    "        model_format = 'transformers',\n",
    "        pooling_strategy = 'reduce_mean',\n",
    "        top_k = k,\n",
    "        max_seq_len = 512,\n",
    "    )\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43133514-6f97-4d21-8fb5-1c2d2ed07360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_retriever(index_name: str, model: str, data: pd.DataFrame, k: int) -> EmbeddingRetriever:\n",
    "    try:\n",
    "        document_store = FAISSDocumentStore(sql_url = f'sqlite:///{index_name}.db', similarity = 'cosine',  embedding_dim = 768)\n",
    "        retriever = get_retriever_scheme(document_store, model, k)\n",
    "        passages_json = [{\"content\": \"passage: \" + row[\"text\"], \"meta\": {\"id\": int(row[\"_id\"])}} for _, row in data.iterrows()]\n",
    "        document_store.write_documents(passages_json)\n",
    "        document_store.update_embeddings(retriever)\n",
    "        document_store.save(index_path = index_name)\n",
    "    except Exception as e:\n",
    "        document_store = FAISSDocumentStore.load(index_name)\n",
    "        retriever = get_retriever_scheme(document_store, model, k)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b48ec0-387e-4019-b8f8-bfd427d5dd3e",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64bfb3-164b-4ebe-833e-92bb0ab39411",
   "metadata": {},
   "source": [
    "Load the documents (passages) from the FiQA corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d20667-c003-4e42-b54d-04582af5191a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>Nie mówię, że nie podoba mi się też pomysł szk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td></td>\n",
       "      <td>Tak więc nic nie zapobiega fałszywym ocenom po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td></td>\n",
       "      <td>Nigdy nie możesz korzystać z FSA dla indywidua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td></td>\n",
       "      <td>Samsung stworzył LCD i inne technologie płaski...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td></td>\n",
       "      <td>Oto wymagania SEC: Federalne przepisy dotycząc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id title                                               text\n",
       "0    3        Nie mówię, że nie podoba mi się też pomysł szk...\n",
       "1   31        Tak więc nic nie zapobiega fałszywym ocenom po...\n",
       "2   56        Nigdy nie możesz korzystać z FSA dla indywidua...\n",
       "3   59        Samsung stworzył LCD i inne technologie płaski...\n",
       "4   63        Oto wymagania SEC: Federalne przepisy dotycząc..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiqa_pl = load_dataset('clarin-knext/fiqa-pl', 'corpus')\n",
    "fiqa_pl_df = fiqa_pl['corpus'].to_pandas()\n",
    "fiqa_pl_df['_id'] = fiqa_pl_df[\"_id\"].apply(lambda x: int(x))\n",
    "fiqa_pl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0270b35-800c-44b8-bf6f-4f97a3d0dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_e5_retriever = set_retriever('multilingual_e5', 'intfloat/multilingual-e5-base', fiqa_pl_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc91df-1474-4870-9768-49a9a2365418",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d0f91d-2c65-4d06-8cef-f2fc267ef623",
   "metadata": {},
   "source": [
    "Use the set of questions and the scorings defined in this corpus, to compute NDCG@5 for the dense retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aafe919-121f-4de8-9b6b-9b1047ef2783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Co jest uważane za wydatek służbowy w podróży ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>Wydatki służbowe - ubezpieczenie samochodu pod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>Rozpoczęcie nowego biznesu online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>„Dzień roboczy” i „termin płatności” rachunków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>Nowy właściciel firmy – Jak działają podatki d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id title                                               text\n",
       "0    0        Co jest uważane za wydatek służbowy w podróży ...\n",
       "1    4        Wydatki służbowe - ubezpieczenie samochodu pod...\n",
       "2    5                        Rozpoczęcie nowego biznesu online\n",
       "3    6           „Dzień roboczy” i „termin płatności” rachunków\n",
       "4    7        Nowy właściciel firmy – Jak działają podatki d..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiqa_pl_queries_df = load_dataset('clarin-knext/fiqa-pl', 'queries')['queries'].to_pandas()\n",
    "fiqa_pl_queries_df['_id'] = fiqa_pl_queries_df['_id'].apply(lambda x: int(x))\n",
    "fiqa_pl_queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64ac4d4-dc76-466a-b934-b35eb07b942f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>566392</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>65404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>325273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>88124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>285255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id  corpus-id  score\n",
       "0         8     566392      1\n",
       "1         8      65404      1\n",
       "2        15     325273      1\n",
       "3        18      88124      1\n",
       "4        26     285255      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiqa_pl_test_df = load_dataset('clarin-knext/fiqa-pl-qrels', 'default')['test'].to_pandas()\n",
    "fiqa_pl_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63edd645-9945-4a79-8c77-16d7e1d91dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ncdgk(answers_ids, revelant_ids, k):\n",
    "    dcg = sum(1 / math.log(i + 2, 2) for i in range(k) if int(answers_ids[i]) in revelant_ids)\n",
    "    idcg = sum(1 / math.log(i + 2, 2) for i in range(min(len(revelant_ids), k)))\n",
    "    \n",
    "    return dcg / idcg\n",
    "\n",
    "def compute_analyzer_mean_ncdgn(retriever, k):\n",
    "    ncdgk_scores = []\n",
    "    \n",
    "    for query_id in tqdm(fiqa_pl_test_df['query-id'].unique(), desc = 'Processing'):\n",
    "        query = fiqa_pl_queries_df[query_id == fiqa_pl_queries_df['_id']].iloc[0]['text'] \n",
    "        answers = retriever.retrieve(query = 'query: ' + query, top_k = k)         \n",
    "        answers_ids = [answer.meta['id'] for answer in answers]\n",
    "        relevant_ids = fiqa_pl_test_df[query_id == fiqa_pl_test_df['query-id']]['corpus-id'].tolist()\n",
    "        ncdgk_score = compute_ncdgk(answers_ids, relevant_ids, k = k)\n",
    "        ncdgk_scores.append(ncdgk_score)\n",
    "\n",
    "    return np.mean(ncdgk_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00ed9a6-658b-40e3-967f-a7b07383e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/648 [00:00<?, ?it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:   0%|          | 1/648 [00:00<03:54,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:   0%|          | 2/648 [00:00<03:59,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:   0%|          | 3/648 [00:01<03:56,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 4/648 [00:01<03:53,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 5/648 [00:01<03:49,  2.80it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 6/648 [00:02<03:49,  2.79it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 7/648 [00:02<03:54,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 8/648 [00:02<03:57,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:   1%|▏         | 9/648 [00:03<04:00,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 10/648 [00:03<04:03,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 11/648 [00:04<04:03,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 12/648 [00:04<04:04,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 13/648 [00:04<04:01,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 14/648 [00:05<03:59,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 15/648 [00:05<03:54,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 16/648 [00:05<03:51,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 17/648 [00:06<03:49,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 18/648 [00:06<03:54,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 19/648 [00:07<03:56,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 20/648 [00:07<03:59,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 21/648 [00:07<03:59,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 22/648 [00:08<03:55,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:   4%|▎         | 23/648 [00:08<03:54,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:   4%|▎         | 24/648 [00:08<03:50,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 25/648 [00:09<03:53,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 26/648 [00:09<03:51,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 27/648 [00:10<03:52,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 28/648 [00:10<03:53,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 29/648 [00:10<03:51,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:   5%|▍         | 30/648 [00:11<03:53,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:   5%|▍         | 31/648 [00:11<03:51,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.51 Batches/s]\u001b[A\n",
      "Processing:   5%|▍         | 32/648 [00:11<03:44,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:   5%|▌         | 33/648 [00:12<03:44,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:   5%|▌         | 34/648 [00:12<03:46,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:   5%|▌         | 35/648 [00:13<03:42,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 36/648 [00:13<03:44,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 37/648 [00:13<03:44,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 38/648 [00:14<03:42,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 39/648 [00:14<03:44,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 40/648 [00:14<03:37,  2.79it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:   6%|▋         | 41/648 [00:15<03:36,  2.80it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:   6%|▋         | 42/648 [00:15<03:36,  2.79it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 43/648 [00:15<03:37,  2.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 44/648 [00:16<03:38,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 45/648 [00:16<03:34,  2.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 46/648 [00:16<03:36,  2.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 47/648 [00:17<03:34,  2.81it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 48/648 [00:17<03:30,  2.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 49/648 [00:18<03:29,  2.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 50/648 [00:18<03:32,  2.81it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 51/648 [00:18<03:40,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 52/648 [00:19<03:40,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 53/648 [00:19<03:38,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 54/648 [00:19<03:39,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 55/648 [00:20<03:41,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:   9%|▊         | 56/648 [00:20<03:39,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 57/648 [00:20<03:35,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 58/648 [00:21<03:31,  2.79it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.54 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 59/648 [00:21<03:26,  2.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.87 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 60/648 [00:21<03:18,  2.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.65 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 61/648 [00:22<03:16,  2.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  10%|▉         | 62/648 [00:22<03:20,  2.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  10%|▉         | 63/648 [00:23<03:23,  2.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  10%|▉         | 64/648 [00:23<03:26,  2.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  10%|█         | 65/648 [00:23<03:25,  2.84it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  10%|█         | 66/648 [00:24<03:24,  2.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n",
      "Processing:  10%|█         | 67/648 [00:24<03:23,  2.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  10%|█         | 68/648 [00:24<03:21,  2.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  11%|█         | 69/648 [00:25<03:19,  2.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.66 Batches/s]\u001b[A\n",
      "Processing:  11%|█         | 70/648 [00:25<03:16,  2.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n",
      "Processing:  11%|█         | 71/648 [00:25<03:16,  2.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n",
      "Processing:  11%|█         | 72/648 [00:26<03:16,  2.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  11%|█▏        | 73/648 [00:26<03:20,  2.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  11%|█▏        | 74/648 [00:26<03:19,  2.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 75/648 [00:27<03:19,  2.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 76/648 [00:27<03:19,  2.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 77/648 [00:27<03:20,  2.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 78/648 [00:28<03:27,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 79/648 [00:28<03:27,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 80/648 [00:29<03:31,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  12%|█▎        | 81/648 [00:29<03:30,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 82/648 [00:29<03:34,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 83/648 [00:30<03:36,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 84/648 [00:30<03:33,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 85/648 [00:30<03:33,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 86/648 [00:31<03:33,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 87/648 [00:31<03:32,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  14%|█▎        | 88/648 [00:32<03:33,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  14%|█▎        | 89/648 [00:32<03:29,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  14%|█▍        | 90/648 [00:32<03:29,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  14%|█▍        | 91/648 [00:33<03:29,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.69 Batches/s]\u001b[A\n",
      "Processing:  14%|█▍        | 92/648 [00:33<03:23,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  14%|█▍        | 93/648 [00:33<03:24,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  15%|█▍        | 94/648 [00:34<03:23,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  15%|█▍        | 95/648 [00:34<03:24,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  15%|█▍        | 96/648 [00:35<03:26,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  15%|█▍        | 97/648 [00:35<03:29,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  15%|█▌        | 98/648 [00:35<03:33,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  15%|█▌        | 99/648 [00:36<03:34,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  15%|█▌        | 100/648 [00:36<03:34,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 101/648 [00:37<03:31,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 102/648 [00:37<03:25,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 103/648 [00:37<03:25,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 104/648 [00:38<03:24,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.75 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 105/648 [00:38<03:17,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n",
      "Processing:  16%|█▋        | 106/648 [00:38<03:16,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 107/648 [00:39<03:17,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 108/648 [00:39<03:19,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 109/648 [00:39<03:19,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 110/648 [00:40<03:20,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.17 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 111/648 [00:40<03:09,  2.83it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 112/648 [00:41<03:15,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 113/648 [00:41<03:17,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 114/648 [00:41<03:19,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 115/648 [00:42<03:21,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 116/648 [00:42<03:24,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 117/648 [00:42<03:25,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 118/648 [00:43<03:26,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 119/648 [00:43<03:26,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  19%|█▊        | 120/648 [00:44<03:21,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  19%|█▊        | 121/648 [00:44<03:18,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 122/648 [00:44<03:20,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 123/648 [00:45<03:17,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 124/648 [00:45<03:15,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 125/648 [00:45<03:14,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 126/648 [00:46<03:17,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.64 Batches/s]\u001b[A\n",
      "Processing:  20%|█▉        | 127/648 [00:46<03:11,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  20%|█▉        | 128/648 [00:47<03:13,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  20%|█▉        | 129/648 [00:47<03:11,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  20%|██        | 130/648 [00:47<03:11,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  20%|██        | 131/648 [00:48<03:12,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.54 Batches/s]\u001b[A\n",
      "Processing:  20%|██        | 132/648 [00:48<03:08,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.85 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 133/648 [00:48<03:02,  2.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 134/648 [00:49<03:04,  2.79it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 135/648 [00:49<03:05,  2.77it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.62 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 136/648 [00:49<03:01,  2.81it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 137/648 [00:50<03:01,  2.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  21%|██▏       | 138/648 [00:50<03:05,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  21%|██▏       | 139/648 [00:51<03:09,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 140/648 [00:51<03:13,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 141/648 [00:51<03:14,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 142/648 [00:52<03:17,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 143/648 [00:52<03:15,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 144/648 [00:53<03:11,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 145/648 [00:53<03:08,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 146/648 [00:53<03:08,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.55 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 147/648 [00:54<03:04,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 148/648 [00:54<03:03,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 149/648 [00:54<03:03,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 150/648 [00:55<03:03,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 151/648 [00:55<03:01,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.54 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 152/648 [00:55<02:59,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  24%|██▎       | 153/648 [00:56<03:01,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.51 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 154/648 [00:56<02:59,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 155/648 [00:57<02:59,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 156/648 [00:57<03:00,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 157/648 [00:57<03:02,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 158/648 [00:58<03:02,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  25%|██▍       | 159/648 [00:58<03:01,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n",
      "Processing:  25%|██▍       | 160/648 [00:58<02:58,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  25%|██▍       | 161/648 [00:59<02:59,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  25%|██▌       | 162/648 [00:59<03:01,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  25%|██▌       | 163/648 [00:59<02:58,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  25%|██▌       | 164/648 [01:00<02:59,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.66 Batches/s]\u001b[A\n",
      "Processing:  25%|██▌       | 165/648 [01:00<02:54,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 166/648 [01:01<02:55,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 167/648 [01:01<02:52,  2.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.70 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 168/648 [01:01<02:50,  2.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 169/648 [01:02<02:53,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.56 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 170/648 [01:02<02:51,  2.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  26%|██▋       | 171/648 [01:02<02:54,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 172/648 [01:03<02:55,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 173/648 [01:03<02:55,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 174/648 [01:04<02:56,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 175/648 [01:04<02:56,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 176/648 [01:04<02:56,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.64 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 177/648 [01:05<02:52,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 178/648 [01:05<02:52,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 179/648 [01:05<02:53,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 180/648 [01:06<02:53,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 181/648 [01:06<02:50,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 182/648 [01:06<02:54,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 183/648 [01:07<02:56,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 184/648 [01:07<02:58,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  29%|██▊       | 185/648 [01:08<02:57,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  29%|██▊       | 186/648 [01:08<02:59,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 187/648 [01:08<02:53,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 188/648 [01:09<02:50,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 189/648 [01:09<02:49,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 190/648 [01:09<02:50,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 191/648 [01:10<02:49,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n",
      "Processing:  30%|██▉       | 192/648 [01:10<02:45,  2.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n",
      "Processing:  30%|██▉       | 193/648 [01:11<02:44,  2.77it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  30%|██▉       | 194/648 [01:11<02:45,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\u001b[A\n",
      "Processing:  30%|███       | 195/648 [01:11<02:42,  2.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  30%|███       | 196/648 [01:12<02:43,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  30%|███       | 197/648 [01:12<02:44,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 198/648 [01:12<02:46,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 199/648 [01:13<02:45,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 200/648 [01:13<02:45,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 201/648 [01:14<02:45,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 202/648 [01:14<02:46,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  31%|███▏      | 203/648 [01:14<02:44,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n",
      "Processing:  31%|███▏      | 204/648 [01:15<02:42,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 205/648 [01:15<02:44,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 206/648 [01:15<02:45,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 207/648 [01:16<02:46,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 208/648 [01:16<02:43,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 209/648 [01:16<02:43,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.73 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 210/648 [01:17<02:38,  2.77it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 211/648 [01:17<02:38,  2.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 212/648 [01:18<02:39,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 213/648 [01:18<02:41,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 214/648 [01:18<02:40,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 215/648 [01:19<02:40,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 216/648 [01:19<02:38,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 217/648 [01:19<02:40,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  34%|███▎      | 218/648 [01:20<02:39,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 219/648 [01:20<02:37,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 220/648 [01:21<02:37,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 221/648 [01:21<02:37,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 222/648 [01:21<02:35,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 223/648 [01:22<02:34,  2.74it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  35%|███▍      | 224/648 [01:22<02:35,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  35%|███▍      | 225/648 [01:22<02:40,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  35%|███▍      | 226/648 [01:23<02:41,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  35%|███▌      | 227/648 [01:23<02:42,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  35%|███▌      | 228/648 [01:24<02:47,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  35%|███▌      | 229/648 [01:24<02:48,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  35%|███▌      | 230/648 [01:24<02:44,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.83 Batches/s]\u001b[A\n",
      "Processing:  36%|███▌      | 231/648 [01:25<02:47,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  36%|███▌      | 232/648 [01:25<02:43,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  36%|███▌      | 233/648 [01:26<02:39,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  36%|███▌      | 234/648 [01:26<02:41,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  36%|███▋      | 235/648 [01:26<02:41,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  36%|███▋      | 236/648 [01:27<02:41,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 237/648 [01:27<02:38,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 238/648 [01:28<02:38,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 239/648 [01:28<02:39,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 240/648 [01:28<02:42,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 241/648 [01:29<02:43,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 242/648 [01:29<02:41,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 243/648 [01:30<02:43,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 244/648 [01:30<02:40,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 245/648 [01:30<02:40,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.84 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 246/648 [01:31<02:43,  2.46it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 247/648 [01:31<02:39,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 248/648 [01:32<02:39,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 249/648 [01:32<02:38,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  39%|███▊      | 250/648 [01:32<02:35,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  39%|███▊      | 251/648 [01:33<02:35,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  39%|███▉      | 252/648 [01:33<02:34,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  39%|███▉      | 253/648 [01:33<02:31,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  39%|███▉      | 254/648 [01:34<02:34,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  39%|███▉      | 255/648 [01:34<02:35,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  40%|███▉      | 256/648 [01:35<02:31,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  40%|███▉      | 257/648 [01:35<02:32,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  40%|███▉      | 258/648 [01:35<02:32,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  40%|███▉      | 259/648 [01:36<02:33,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  40%|████      | 260/648 [01:36<02:32,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  40%|████      | 261/648 [01:37<02:33,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  40%|████      | 262/648 [01:37<02:30,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 263/648 [01:37<02:31,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 264/648 [01:38<02:30,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 265/648 [01:38<02:31,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 266/648 [01:39<02:32,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 267/648 [01:39<02:35,  2.46it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  41%|████▏     | 268/648 [01:39<02:35,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 269/648 [01:40<02:32,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 270/648 [01:40<02:33,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 271/648 [01:41<02:29,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 272/648 [01:41<02:26,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 273/648 [01:41<02:27,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 274/648 [01:42<02:28,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 275/648 [01:42<02:27,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 276/648 [01:43<02:27,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 277/648 [01:43<02:25,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 278/648 [01:43<02:24,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 279/648 [01:44<02:24,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 280/648 [01:44<02:22,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 281/648 [01:44<02:19,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  44%|████▎     | 282/648 [01:45<02:18,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.54 Batches/s]\u001b[A\n",
      "Processing:  44%|████▎     | 283/648 [01:45<02:15,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 284/648 [01:46<02:14,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 285/648 [01:46<02:15,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 286/648 [01:46<02:15,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 287/648 [01:47<02:14,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 288/648 [01:47<02:12,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  45%|████▍     | 289/648 [01:47<02:11,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  45%|████▍     | 290/648 [01:48<02:12,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  45%|████▍     | 291/648 [01:48<02:14,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  45%|████▌     | 292/648 [01:49<02:12,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  45%|████▌     | 293/648 [01:49<02:12,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  45%|████▌     | 294/648 [01:49<02:12,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 295/648 [01:50<02:12,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 296/648 [01:50<02:12,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 297/648 [01:50<02:10,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 298/648 [01:51<02:08,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 299/648 [01:51<02:10,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  46%|████▋     | 300/648 [01:52<02:10,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  46%|████▋     | 301/648 [01:52<02:12,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 302/648 [01:52<02:13,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 303/648 [01:53<02:12,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 304/648 [01:53<02:11,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 305/648 [01:53<02:09,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 306/648 [01:54<02:06,  2.69it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 307/648 [01:54<02:10,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 308/648 [01:55<02:10,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 309/648 [01:55<02:13,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 310/648 [01:55<02:13,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 311/648 [01:56<02:13,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 312/648 [01:56<02:14,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 313/648 [01:57<02:11,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 314/648 [01:57<02:09,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  49%|████▊     | 315/648 [01:57<02:07,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 316/648 [01:58<02:06,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 317/648 [01:58<02:06,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 318/648 [01:58<02:05,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 319/648 [01:59<02:02,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 320/648 [01:59<02:04,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  50%|████▉     | 321/648 [02:00<02:03,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  50%|████▉     | 322/648 [02:00<02:02,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  50%|████▉     | 323/648 [02:00<02:03,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  50%|█████     | 324/648 [02:01<02:01,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  50%|█████     | 325/648 [02:01<02:02,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  50%|█████     | 326/648 [02:02<02:02,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  50%|█████     | 327/648 [02:02<02:01,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 328/648 [02:02<02:00,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 329/648 [02:03<02:00,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 330/648 [02:03<02:00,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 331/648 [02:03<01:59,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 332/648 [02:04<01:56,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  51%|█████▏    | 333/648 [02:04<01:57,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 334/648 [02:05<01:58,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 335/648 [02:05<01:58,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 336/648 [02:05<01:57,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 337/648 [02:06<01:58,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.82 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 338/648 [02:06<01:53,  2.73it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 339/648 [02:06<01:53,  2.72it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 340/648 [02:07<01:53,  2.71it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 341/648 [02:07<01:54,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 342/648 [02:08<01:55,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 343/648 [02:08<01:56,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 344/648 [02:08<01:55,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 345/648 [02:09<01:55,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 346/648 [02:09<01:52,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▎    | 347/648 [02:09<01:51,  2.70it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▎    | 348/648 [02:10<01:52,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 349/648 [02:10<01:54,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 350/648 [02:11<01:53,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 351/648 [02:11<01:55,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 352/648 [02:11<01:57,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 353/648 [02:12<01:57,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▍    | 354/648 [02:12<01:58,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▍    | 355/648 [02:13<01:56,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▍    | 356/648 [02:13<01:54,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▌    | 357/648 [02:13<01:53,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▌    | 358/648 [02:14<01:53,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▌    | 359/648 [02:14<01:52,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 360/648 [02:14<01:50,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 361/648 [02:15<01:50,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 362/648 [02:15<01:46,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 363/648 [02:16<01:49,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 364/648 [02:16<01:48,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▋    | 365/648 [02:16<01:48,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▋    | 366/648 [02:17<01:50,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 367/648 [02:17<01:49,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 368/648 [02:18<01:48,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 369/648 [02:18<01:48,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 370/648 [02:18<01:46,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 371/648 [02:19<01:46,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 372/648 [02:19<01:46,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 373/648 [02:20<01:46,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 374/648 [02:20<01:45,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 375/648 [02:20<01:45,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 376/648 [02:21<01:46,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 377/648 [02:21<01:44,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 378/648 [02:21<01:41,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 379/648 [02:22<01:40,  2.67it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▊    | 380/648 [02:22<01:40,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 381/648 [02:23<01:41,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 382/648 [02:23<01:39,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 383/648 [02:23<01:40,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 384/648 [02:24<01:40,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 385/648 [02:24<01:41,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  60%|█████▉    | 386/648 [02:24<01:40,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  60%|█████▉    | 387/648 [02:25<01:39,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  60%|█████▉    | 388/648 [02:25<01:37,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  60%|██████    | 389/648 [02:26<01:38,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  60%|██████    | 390/648 [02:26<01:38,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.84 Batches/s]\u001b[A\n",
      "Processing:  60%|██████    | 391/648 [02:26<01:41,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n",
      "Processing:  60%|██████    | 392/648 [02:27<01:43,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:  61%|██████    | 393/648 [02:27<01:44,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  61%|██████    | 394/648 [02:28<01:43,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n",
      "Processing:  61%|██████    | 395/648 [02:28<01:44,  2.42it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  61%|██████    | 396/648 [02:28<01:43,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  61%|██████▏   | 397/648 [02:29<01:43,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  61%|██████▏   | 398/648 [02:29<01:42,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 399/648 [02:30<01:41,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 400/648 [02:30<01:40,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 401/648 [02:31<01:41,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 402/648 [02:31<01:40,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 403/648 [02:31<01:39,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 404/648 [02:32<01:38,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▎   | 405/648 [02:32<01:37,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 406/648 [02:33<01:37,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 407/648 [02:33<01:36,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 408/648 [02:33<01:35,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 409/648 [02:34<01:34,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 410/648 [02:34<01:32,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 411/648 [02:34<01:32,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▎   | 412/648 [02:35<01:32,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▎   | 413/648 [02:35<01:32,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▍   | 414/648 [02:36<01:31,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▍   | 415/648 [02:36<01:29,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▍   | 416/648 [02:36<01:27,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▍   | 417/648 [02:37<01:30,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▍   | 418/648 [02:37<01:28,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▍   | 419/648 [02:38<01:27,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▍   | 420/648 [02:38<01:25,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▍   | 421/648 [02:38<01:26,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▌   | 422/648 [02:39<01:27,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▌   | 423/648 [02:39<01:27,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▌   | 424/648 [02:39<01:27,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 425/648 [02:40<01:28,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 426/648 [02:40<01:27,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 427/648 [02:41<01:26,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 428/648 [02:41<01:25,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.77 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 429/648 [02:41<01:27,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▋   | 430/648 [02:42<01:27,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.88 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 431/648 [02:42<01:28,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.86 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 432/648 [02:43<01:28,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.77 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 433/648 [02:43<01:31,  2.35it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.84 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 434/648 [02:44<01:31,  2.35it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 435/648 [02:44<01:30,  2.35it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 436/648 [02:44<01:29,  2.38it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 437/648 [02:45<01:25,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 438/648 [02:45<01:22,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 439/648 [02:46<01:23,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 440/648 [02:46<01:21,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 441/648 [02:46<01:19,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 442/648 [02:47<01:19,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 443/648 [02:47<01:17,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▊   | 444/648 [02:47<01:16,  2.68it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▊   | 445/648 [02:48<01:16,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 446/648 [02:48<01:16,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 447/648 [02:49<01:16,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 448/648 [02:49<01:16,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 449/648 [02:49<01:17,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 450/648 [02:50<01:16,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  70%|██████▉   | 451/648 [02:50<01:16,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  70%|██████▉   | 452/648 [02:51<01:15,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  70%|██████▉   | 453/648 [02:51<01:16,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  70%|███████   | 454/648 [02:51<01:14,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  70%|███████   | 455/648 [02:52<01:14,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  70%|███████   | 456/648 [02:52<01:13,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 457/648 [02:52<01:12,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 458/648 [02:53<01:12,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 459/648 [02:53<01:12,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 460/648 [02:54<01:11,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 461/648 [02:54<01:10,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  71%|███████▏  | 462/648 [02:54<01:10,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  71%|███████▏  | 463/648 [02:55<01:10,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 464/648 [02:55<01:09,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 465/648 [02:55<01:09,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 466/648 [02:56<01:09,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 467/648 [02:56<01:09,  2.61it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 468/648 [02:57<01:08,  2.64it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 469/648 [02:57<01:07,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 470/648 [02:57<01:07,  2.66it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 471/648 [02:58<01:07,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 472/648 [02:58<01:08,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 473/648 [02:59<01:08,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 474/648 [02:59<01:08,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 475/648 [02:59<01:09,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 476/648 [03:00<01:09,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.76 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▎  | 477/648 [03:00<01:10,  2.42it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 478/648 [03:01<01:08,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 479/648 [03:01<01:07,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 480/648 [03:01<01:05,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 481/648 [03:02<01:05,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 482/648 [03:02<01:04,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▍  | 483/648 [03:03<01:04,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▍  | 484/648 [03:03<01:03,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▍  | 485/648 [03:03<01:03,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▌  | 486/648 [03:04<01:02,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▌  | 487/648 [03:04<01:02,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▌  | 488/648 [03:04<01:02,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▌  | 489/648 [03:05<01:02,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 490/648 [03:05<01:01,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 491/648 [03:06<01:00,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 492/648 [03:06<01:00,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 493/648 [03:06<01:00,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 494/648 [03:07<00:59,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▋  | 495/648 [03:07<00:59,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 496/648 [03:08<00:59,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 497/648 [03:08<00:58,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 498/648 [03:08<00:57,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 499/648 [03:09<00:57,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 500/648 [03:09<00:55,  2.65it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 501/648 [03:09<00:55,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 502/648 [03:10<00:56,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 503/648 [03:10<00:56,  2.59it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 504/648 [03:11<00:56,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 505/648 [03:11<00:55,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 506/648 [03:11<00:55,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 507/648 [03:12<00:54,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 508/648 [03:12<00:54,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▊  | 509/648 [03:13<00:54,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▊  | 510/648 [03:13<00:52,  2.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 511/648 [03:13<00:53,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 512/648 [03:14<00:52,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 513/648 [03:14<00:53,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 514/648 [03:15<00:52,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 515/648 [03:15<00:53,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  80%|███████▉  | 516/648 [03:15<00:53,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.76 Batches/s]\u001b[A\n",
      "Processing:  80%|███████▉  | 517/648 [03:16<00:54,  2.42it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  80%|███████▉  | 518/648 [03:16<00:53,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  80%|████████  | 519/648 [03:17<00:51,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  80%|████████  | 520/648 [03:17<00:51,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  80%|████████  | 521/648 [03:17<00:50,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 522/648 [03:18<00:50,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 523/648 [03:18<00:48,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 524/648 [03:19<00:48,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 525/648 [03:19<00:47,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 526/648 [03:19<00:47,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  81%|████████▏ | 527/648 [03:20<00:47,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  81%|████████▏ | 528/648 [03:20<00:46,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 529/648 [03:20<00:45,  2.60it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 530/648 [03:21<00:46,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 531/648 [03:21<00:46,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 532/648 [03:22<00:45,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 533/648 [03:22<00:45,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 534/648 [03:22<00:44,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 535/648 [03:23<00:44,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 536/648 [03:23<00:43,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 537/648 [03:24<00:42,  2.62it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 538/648 [03:24<00:42,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 539/648 [03:24<00:42,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 540/648 [03:25<00:42,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 541/648 [03:25<00:41,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▎ | 542/648 [03:26<00:41,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 543/648 [03:26<00:40,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 544/648 [03:26<00:40,  2.58it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 545/648 [03:27<00:40,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 546/648 [03:27<00:39,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 547/648 [03:28<00:39,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▍ | 548/648 [03:28<00:39,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▍ | 549/648 [03:28<00:38,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▍ | 550/648 [03:29<00:38,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▌ | 551/648 [03:29<00:37,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▌ | 552/648 [03:29<00:37,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▌ | 553/648 [03:30<00:37,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▌ | 554/648 [03:30<00:37,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▌ | 555/648 [03:31<00:37,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▌ | 556/648 [03:31<00:37,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.84 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▌ | 557/648 [03:32<00:37,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.83 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▌ | 558/648 [03:32<00:37,  2.42it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▋ | 559/648 [03:32<00:36,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▋ | 560/648 [03:33<00:35,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 561/648 [03:33<00:35,  2.46it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 562/648 [03:34<00:34,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 563/648 [03:34<00:34,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 564/648 [03:34<00:33,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 565/648 [03:35<00:32,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 566/648 [03:35<00:32,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 567/648 [03:36<00:32,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 568/648 [03:36<00:31,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 569/648 [03:36<00:31,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 570/648 [03:37<00:31,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 571/648 [03:37<00:30,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 572/648 [03:38<00:29,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 573/648 [03:38<00:29,  2.57it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▊ | 574/648 [03:38<00:28,  2.56it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▊ | 575/648 [03:39<00:28,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▉ | 576/648 [03:39<00:28,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▉ | 577/648 [03:40<00:28,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▉ | 578/648 [03:40<00:27,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▉ | 579/648 [03:40<00:27,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  90%|████████▉ | 580/648 [03:41<00:26,  2.55it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  90%|████████▉ | 581/648 [03:41<00:26,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  90%|████████▉ | 582/648 [03:41<00:26,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  90%|████████▉ | 583/648 [03:42<00:26,  2.46it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  90%|█████████ | 584/648 [03:42<00:25,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  90%|█████████ | 585/648 [03:43<00:25,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.88 Batches/s]\u001b[A\n",
      "Processing:  90%|█████████ | 586/648 [03:43<00:25,  2.46it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.86 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 587/648 [03:44<00:25,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 588/648 [03:44<00:24,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 589/648 [03:44<00:23,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 590/648 [03:45<00:22,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 591/648 [03:45<00:22,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████▏| 592/648 [03:46<00:22,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 593/648 [03:46<00:22,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 594/648 [03:46<00:22,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 595/648 [03:47<00:21,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.81 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 596/648 [03:47<00:21,  2.41it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.73 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 597/648 [03:48<00:21,  2.37it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.79 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 598/648 [03:48<00:21,  2.36it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 599/648 [03:48<00:20,  2.39it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 600/648 [03:49<00:19,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 601/648 [03:49<00:19,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 602/648 [03:50<00:18,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 603/648 [03:50<00:18,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 604/648 [03:50<00:17,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 605/648 [03:51<00:17,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▎| 606/648 [03:51<00:16,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▎| 607/648 [03:52<00:16,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 608/648 [03:52<00:16,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 609/648 [03:52<00:15,  2.54it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 610/648 [03:53<00:15,  2.53it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 611/648 [03:53<00:14,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 612/648 [03:54<00:14,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▍| 613/648 [03:54<00:13,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▍| 614/648 [03:54<00:13,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▍| 615/648 [03:55<00:13,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▌| 616/648 [03:55<00:12,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▌| 617/648 [03:56<00:12,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▌| 618/648 [03:56<00:11,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 619/648 [03:56<00:11,  2.51it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 620/648 [03:57<00:11,  2.50it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.72 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 621/648 [03:57<00:11,  2.41it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 622/648 [03:58<00:10,  2.48it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 623/648 [03:58<00:09,  2.52it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▋| 624/648 [03:58<00:09,  2.46it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▋| 625/648 [03:59<00:09,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 626/648 [03:59<00:08,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 627/648 [04:00<00:08,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 628/648 [04:00<00:08,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 629/648 [04:00<00:07,  2.49it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 630/648 [04:01<00:07,  2.47it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 631/648 [04:01<00:06,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 632/648 [04:02<00:06,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 633/648 [04:02<00:06,  2.46it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 634/648 [04:03<00:05,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.88 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 635/648 [04:03<00:05,  2.40it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 636/648 [04:03<00:04,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 637/648 [04:04<00:04,  2.42it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 638/648 [04:04<00:04,  2.42it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▊| 639/648 [04:05<00:03,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 640/648 [04:05<00:03,  2.44it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 641/648 [04:05<00:02,  2.40it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 642/648 [04:06<00:02,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 643/648 [04:06<00:02,  2.45it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 644/648 [04:07<00:01,  2.46it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.91 Batches/s]\u001b[A\n",
      "Processing: 100%|█████████▉| 645/648 [04:07<00:01,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing: 100%|█████████▉| 646/648 [04:07<00:00,  2.41it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing: 100%|█████████▉| 647/648 [04:08<00:00,  2.43it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing: 100%|██████████| 648/648 [04:08<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ndgc5_dense_retriever = compute_analyzer_mean_ncdgn(multilingual_e5_retriever, 5)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1c066a8-28ef-4198-b093-487cdbc8e6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NCDG@5 for the dense retriever: 0.23468295908732414\n",
      "Time: 248.8029489517212\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean NCDG@5 for the dense retriever: {ndgc5_dense_retriever}\\nTime: {total_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15039e77-4b8e-47b7-8c61-4cdadae68756",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebead53-02ea-49cf-8c28-67199b9240e2",
   "metadata": {},
   "source": [
    "Compare the NDCG score from this exercise with the score from [lab 2](2-fts.md) and from [lab 6](6-classification.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b4db4-d806-45b2-953d-2a88ccb8110b",
   "metadata": {},
   "source": [
    "The NDCG@5 score obtained in Task 5 is higher than the scores obtained in Lab 2 and Lab 6 (my Lab 5 this year). In my opinion, it is a very good result (about 0.2347), better than the result obtained for Elasticsearch (0.1851) in Lab 2 and the result obtained for the trained model in Lab 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95651fd0-fb91-4c02-a874-f45c3258bc31",
   "metadata": {},
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b6cb6-5e99-4764-bf3f-14f542b85a51",
   "metadata": {},
   "source": [
    "**Bonus** (+2p) Combine dense retrieval with classification model from [lab 6](6-classification.md) to implement a two-step\r\n",
    "   retrieval. Compute NDCG@5 for this combined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f36a42c-40bc-4e12-aba8-0683c0d3e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analyzer_mean_ncdgn(trainer, tokenizer, retriever, k):\n",
    "    ncdgk_scores = []\n",
    "    \n",
    "    for query_id in tqdm(fiqa_pl_test_df['query-id'].unique(), desc = 'Processing'):\n",
    "        query = fiqa_pl_queries_df[query_id == fiqa_pl_queries_df['_id']].iloc[0]['text']\n",
    "        answers = retriever.retrieve(query = 'query: ' + query, top_k = k)         \n",
    "        answers_ids = [answer.meta['id'] for answer in answers]\n",
    "        query_answer_df = pd.DataFrame([[query, fiqa_pl_df[fiqa_pl_df['_id'] == answer_id]['text'].values[0]] for answer_id in answers_ids\n",
    "                                       if len(fiqa_pl_df[fiqa_pl_df['_id'] == answer_id]['text'].values) != 0], columns = ['query', 'answer'])\n",
    "        dataset = Dataset.from_pandas(query_answer_df)\n",
    "        tokenized_dataset = dataset.map(lambda example: tokenizer(example['query'], example['answer'], truncation = True, padding = 'max_length'))\n",
    "        predictions = trainer.predict(tokenized_dataset)\n",
    "        predictions = softmax(predictions.predictions, axis = 1)\n",
    "        predictions_with_ids = sorted(zip(predictions.tolist(), answers_ids), key = lambda x: x[0][1], reverse = True)\n",
    "        top5_predictions_with_ids = [element[1] for element in predictions_with_ids][:5]\n",
    "        relevant_ids = fiqa_pl_test_df[query_id == fiqa_pl_test_df['query-id']]['corpus-id'].tolist()\n",
    "        ncdgk_score = compute_ncdgk(top5_predictions_with_ids, relevant_ids, k = k)\n",
    "        ncdgk_scores.append(ncdgk_score)\n",
    "\n",
    "    return np.mean(ncdgk_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35cd77af-143f-478f-a734-3c8f167fdcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/648 [00:00<?, ?it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 1/648 [00:01<21:09,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 2/648 [00:03<21:14,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 3/648 [00:05<21:22,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 4/648 [00:07<20:40,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 5/648 [00:09<20:29,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 6/648 [00:11<20:04,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 7/648 [00:13<20:10,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 8/648 [00:15<20:30,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|▏         | 9/648 [00:17<20:23,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 10/648 [00:19<20:08,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 11/648 [00:21<20:13,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 12/648 [00:22<19:56,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 13/648 [00:24<19:43,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 14/648 [00:26<19:27,  1.84s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 15/648 [00:28<19:21,  1.83s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 16/648 [00:30<19:19,  1.84s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 17/648 [00:31<19:01,  1.81s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 18/648 [00:33<19:27,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 19/648 [00:35<19:46,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 20/648 [00:37<20:08,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 21/648 [00:39<20:04,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 22/648 [00:41<20:01,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▎         | 23/648 [00:43<20:01,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▎         | 24/648 [00:45<20:05,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 25/648 [00:47<19:57,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 26/648 [00:49<19:53,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 27/648 [00:51<19:53,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 28/648 [00:53<20:02,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|▍         | 29/648 [00:55<19:55,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▍         | 30/648 [00:57<19:39,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▍         | 31/648 [00:58<19:35,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▍         | 32/648 [01:00<19:27,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▌         | 33/648 [01:02<19:23,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▌         | 34/648 [01:04<19:19,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.55 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   5%|▌         | 35/648 [01:06<19:02,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▌         | 36/648 [01:08<19:14,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▌         | 37/648 [01:10<19:19,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▌         | 38/648 [01:12<19:16,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▌         | 39/648 [01:13<19:08,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▌         | 40/648 [01:15<19:26,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▋         | 41/648 [01:17<19:19,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▋         | 42/648 [01:19<19:06,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 43/648 [01:21<18:57,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 44/648 [01:23<18:44,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 45/648 [01:25<19:05,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 46/648 [01:27<19:02,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.71 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 47/648 [01:29<18:42,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|▋         | 48/648 [01:30<18:45,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 49/648 [01:32<18:42,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 50/648 [01:34<18:36,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 51/648 [01:36<18:37,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.69 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 52/648 [01:38<18:29,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 53/648 [01:40<18:46,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 54/648 [01:42<18:43,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   8%|▊         | 55/648 [01:44<18:33,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▊         | 56/648 [01:45<18:32,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 57/648 [01:47<18:43,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 58/648 [01:49<18:42,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 59/648 [01:51<18:40,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 60/648 [01:53<18:38,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.55 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   9%|▉         | 61/648 [01:55<18:30,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|▉         | 62/648 [01:57<18:47,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|▉         | 63/648 [01:59<18:32,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|▉         | 64/648 [02:01<18:28,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█         | 65/648 [02:03<18:16,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█         | 66/648 [02:04<18:07,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█         | 67/648 [02:06<18:01,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|█         | 68/648 [02:08<17:51,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  11%|█         | 69/648 [02:10<17:54,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.64 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  11%|█         | 70/648 [02:12<18:07,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  11%|█         | 71/648 [02:14<18:17,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  11%|█         | 72/648 [02:16<18:28,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  11%|█▏        | 73/648 [02:18<18:16,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  11%|█▏        | 74/648 [02:20<18:07,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 75/648 [02:21<17:58,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 76/648 [02:23<17:52,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 77/648 [02:25<17:52,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 78/648 [02:27<17:59,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 79/648 [02:29<18:04,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▏        | 80/648 [02:31<18:01,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  12%|█▎        | 81/648 [02:33<17:56,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 82/648 [02:35<17:42,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 83/648 [02:37<17:36,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 84/648 [02:38<17:34,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 85/648 [02:40<17:28,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 86/648 [02:42<17:22,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 87/648 [02:44<17:32,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  14%|█▎        | 88/648 [02:46<17:37,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  14%|█▎        | 89/648 [02:48<17:25,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  14%|█▍        | 90/648 [02:50<17:28,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  14%|█▍        | 91/648 [02:51<17:17,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  14%|█▍        | 92/648 [02:53<17:24,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  14%|█▍        | 93/648 [02:55<17:22,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▍        | 94/648 [02:57<17:19,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▍        | 95/648 [02:59<17:22,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▍        | 96/648 [03:01<17:34,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▍        | 97/648 [03:03<17:26,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▌        | 98/648 [03:05<17:19,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▌        | 99/648 [03:07<17:13,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  15%|█▌        | 100/648 [03:08<17:09,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.83 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▌        | 101/648 [03:10<16:51,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▌        | 102/648 [03:12<16:57,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▌        | 103/648 [03:14<17:04,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▌        | 104/648 [03:16<17:28,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▌        | 105/648 [03:18<17:24,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▋        | 106/648 [03:20<17:10,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 107/648 [03:22<16:51,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 108/648 [03:24<16:51,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 109/648 [03:25<16:47,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.66 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 110/648 [03:27<16:42,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 111/648 [03:29<16:38,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.66 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 112/648 [03:31<16:40,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 113/648 [03:33<16:58,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 114/648 [03:35<16:47,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 115/648 [03:37<16:46,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 116/648 [03:39<16:36,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 117/648 [03:40<16:34,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 118/648 [03:42<16:30,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  18%|█▊        | 119/648 [03:44<16:27,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▊        | 120/648 [03:46<16:23,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▊        | 121/648 [03:48<16:39,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 122/648 [03:50<16:39,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 123/648 [03:52<16:31,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 124/648 [03:54<16:25,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 125/648 [03:55<16:22,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 126/648 [03:57<16:21,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|█▉        | 127/648 [03:59<16:20,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|█▉        | 128/648 [04:01<16:22,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|█▉        | 129/648 [04:03<16:24,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 130/648 [04:05<16:34,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 131/648 [04:07<16:27,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|██        | 132/648 [04:09<16:21,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██        | 133/648 [04:11<16:12,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██        | 134/648 [04:13<16:09,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██        | 135/648 [04:14<16:15,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██        | 136/648 [04:16<16:26,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██        | 137/648 [04:18<16:11,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██▏       | 138/648 [04:20<16:18,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  21%|██▏       | 139/648 [04:22<16:06,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 140/648 [04:24<15:54,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 141/648 [04:26<15:51,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 142/648 [04:28<15:48,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 143/648 [04:30<15:40,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 144/648 [04:31<15:40,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  22%|██▏       | 145/648 [04:33<15:31,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 146/648 [04:35<15:35,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 147/648 [04:37<15:47,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 148/648 [04:39<15:41,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 149/648 [04:41<15:37,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 150/648 [04:43<15:29,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 151/648 [04:44<15:19,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 152/648 [04:46<15:20,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|██▎       | 153/648 [04:48<15:25,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|██▍       | 154/648 [04:50<15:21,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|██▍       | 155/648 [04:52<15:31,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|██▍       | 156/648 [04:54<15:30,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|██▍       | 157/648 [04:56<15:26,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|██▍       | 158/648 [04:58<15:15,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▍       | 159/648 [04:59<15:08,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.74 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▍       | 160/648 [05:01<15:04,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▍       | 161/648 [05:03<15:00,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 162/648 [05:05<15:10,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 163/648 [05:07<15:15,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 164/648 [05:09<15:35,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 165/648 [05:11<15:22,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▌       | 166/648 [05:13<15:19,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▌       | 167/648 [05:15<15:24,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▌       | 168/648 [05:17<15:48,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▌       | 169/648 [05:19<15:45,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▌       | 170/648 [05:21<15:56,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▋       | 171/648 [05:23<15:58,  2.01s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.81 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 172/648 [05:25<16:14,  2.05s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 173/648 [05:27<16:02,  2.03s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 174/648 [05:29<15:44,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 175/648 [05:31<15:23,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 176/648 [05:33<15:11,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 177/648 [05:35<15:00,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|██▋       | 178/648 [05:36<15:00,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 179/648 [05:38<15:11,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 180/648 [05:41<15:38,  2.01s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 181/648 [05:43<15:33,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 182/648 [05:45<15:22,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 183/648 [05:47<15:18,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 184/648 [05:48<15:03,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▊       | 185/648 [05:50<15:13,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▊       | 186/648 [05:52<15:00,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.56 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▉       | 187/648 [05:54<14:44,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▉       | 188/648 [05:56<14:49,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▉       | 189/648 [05:58<15:01,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.82 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▉       | 190/648 [06:00<15:06,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▉       | 191/648 [06:02<14:53,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|██▉       | 192/648 [06:04<14:44,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|██▉       | 193/648 [06:06<14:32,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|██▉       | 194/648 [06:08<14:23,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.51 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 195/648 [06:10<14:11,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 196/648 [06:12<14:19,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 197/648 [06:14<14:36,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███       | 198/648 [06:16<14:39,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███       | 199/648 [06:17<14:24,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███       | 200/648 [06:19<14:16,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███       | 201/648 [06:21<14:09,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███       | 202/648 [06:23<14:01,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███▏      | 203/648 [06:25<13:59,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  31%|███▏      | 204/648 [06:27<13:54,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 205/648 [06:29<14:10,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 206/648 [06:31<13:59,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 207/648 [06:32<13:56,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 208/648 [06:34<13:48,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.54 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 209/648 [06:36<13:41,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 210/648 [06:38<13:37,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 211/648 [06:40<13:29,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 212/648 [06:42<13:31,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 213/648 [06:44<13:40,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 214/648 [06:46<13:43,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 215/648 [06:48<13:40,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 216/648 [06:49<13:35,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 217/648 [06:51<13:35,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▎      | 218/648 [06:53<13:24,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▍      | 219/648 [06:55<13:17,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▍      | 220/648 [06:57<13:12,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▍      | 221/648 [06:59<13:11,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▍      | 222/648 [07:01<13:22,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  34%|███▍      | 223/648 [07:02<13:22,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▍      | 224/648 [07:04<13:19,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▍      | 225/648 [07:06<13:17,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▍      | 226/648 [07:08<13:10,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 227/648 [07:10<13:07,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 228/648 [07:12<13:05,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 229/648 [07:14<13:07,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 230/648 [07:16<13:22,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▌      | 231/648 [07:18<13:33,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▌      | 232/648 [07:20<13:27,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▌      | 233/648 [07:22<13:18,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▌      | 234/648 [07:23<13:02,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▋      | 235/648 [07:25<12:57,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▋      | 236/648 [07:27<12:44,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 237/648 [07:29<12:42,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 238/648 [07:31<12:41,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 239/648 [07:33<12:55,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 240/648 [07:35<12:48,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 241/648 [07:36<12:41,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|███▋      | 242/648 [07:38<12:33,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 243/648 [07:40<12:35,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 244/648 [07:42<12:28,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 245/648 [07:44<12:26,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 246/648 [07:46<12:27,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 247/648 [07:48<12:30,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 248/648 [07:50<12:41,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 249/648 [07:52<12:46,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▊      | 250/648 [07:53<12:36,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▊      | 251/648 [07:55<12:30,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▉      | 252/648 [07:57<12:28,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▉      | 253/648 [07:59<12:23,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▉      | 254/648 [08:01<12:15,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▉      | 255/648 [08:03<12:16,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.84 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|███▉      | 256/648 [08:05<12:31,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|███▉      | 257/648 [08:07<12:21,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|███▉      | 258/648 [08:08<12:19,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|███▉      | 259/648 [08:10<12:12,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|████      | 260/648 [08:12<12:13,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|████      | 261/648 [08:14<12:13,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|████      | 262/648 [08:16<12:19,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████      | 263/648 [08:18<12:16,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████      | 264/648 [08:20<12:23,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████      | 265/648 [08:22<12:21,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████      | 266/648 [08:24<12:21,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████      | 267/648 [08:26<12:18,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  41%|████▏     | 268/648 [08:28<12:10,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 269/648 [08:30<12:00,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 270/648 [08:31<11:52,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 271/648 [08:33<11:46,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 272/648 [08:35<11:42,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 273/648 [08:37<11:53,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 274/648 [08:39<11:46,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 275/648 [08:41<11:36,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 276/648 [08:43<11:33,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 277/648 [08:44<11:30,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 278/648 [08:46<11:25,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 279/648 [08:48<11:26,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 280/648 [08:50<11:31,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 281/648 [08:52<11:45,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▎     | 282/648 [08:54<11:47,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▎     | 283/648 [08:56<11:35,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▍     | 284/648 [08:58<11:27,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▍     | 285/648 [09:00<11:25,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▍     | 286/648 [09:01<11:15,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▍     | 287/648 [09:03<11:06,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  44%|████▍     | 288/648 [09:05<11:08,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▍     | 289/648 [09:07<11:11,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▍     | 290/648 [09:09<11:17,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▍     | 291/648 [09:11<11:11,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▌     | 292/648 [09:13<11:07,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▌     | 293/648 [09:15<11:11,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▌     | 294/648 [09:17<11:13,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▌     | 295/648 [09:18<11:07,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▌     | 296/648 [09:20<11:02,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▌     | 297/648 [09:22<10:58,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▌     | 298/648 [09:24<11:07,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▌     | 299/648 [09:26<11:08,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▋     | 300/648 [09:28<10:59,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  46%|████▋     | 301/648 [09:30<11:03,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 302/648 [09:32<11:02,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 303/648 [09:34<10:57,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 304/648 [09:36<10:48,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 305/648 [09:37<10:41,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 306/648 [09:39<10:43,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|████▋     | 307/648 [09:41<10:47,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 308/648 [09:43<10:46,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 309/648 [09:45<10:38,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 310/648 [09:47<10:32,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.51 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 311/648 [09:49<10:31,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.51 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 312/648 [09:51<10:24,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 313/648 [09:52<10:27,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 314/648 [09:54<10:23,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▊     | 315/648 [09:56<10:29,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▉     | 316/648 [09:58<10:29,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▉     | 317/648 [10:00<10:24,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▉     | 318/648 [10:02<10:24,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▉     | 319/648 [10:04<10:23,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  49%|████▉     | 320/648 [10:06<10:16,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|████▉     | 321/648 [10:08<10:21,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|████▉     | 322/648 [10:09<10:18,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|████▉     | 323/648 [10:11<10:22,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 324/648 [10:13<10:28,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 325/648 [10:15<10:29,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 326/648 [10:17<10:25,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 327/648 [10:19<10:14,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  51%|█████     | 328/648 [10:21<10:07,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  51%|█████     | 329/648 [10:23<09:59,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  51%|█████     | 330/648 [10:25<09:54,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  51%|█████     | 331/648 [10:27<09:53,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  51%|█████     | 332/648 [10:29<10:00,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  51%|█████▏    | 333/648 [10:30<09:55,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 334/648 [10:32<09:48,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 335/648 [10:34<09:51,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 336/648 [10:36<09:53,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 337/648 [10:38<09:55,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 338/648 [10:40<09:49,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 339/648 [10:42<09:44,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 340/648 [10:44<09:48,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 341/648 [10:46<09:50,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 342/648 [10:48<09:38,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 343/648 [10:49<09:38,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 344/648 [10:51<09:34,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 345/648 [10:53<09:30,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  53%|█████▎    | 346/648 [10:55<09:24,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▎    | 347/648 [10:57<09:22,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▎    | 348/648 [10:59<09:26,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 349/648 [11:01<09:32,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 350/648 [11:03<09:31,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 351/648 [11:05<09:27,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 352/648 [11:06<09:18,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  54%|█████▍    | 353/648 [11:08<09:23,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▍    | 354/648 [11:10<09:23,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▍    | 355/648 [11:12<09:17,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▍    | 356/648 [11:14<09:13,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▌    | 357/648 [11:16<09:25,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▌    | 358/648 [11:18<09:23,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▌    | 359/648 [11:20<09:17,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▌    | 360/648 [11:22<09:03,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▌    | 361/648 [11:24<09:00,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▌    | 362/648 [11:25<08:54,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▌    | 363/648 [11:27<08:51,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▌    | 364/648 [11:29<08:41,  1.84s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.56 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▋    | 365/648 [11:31<08:35,  1.82s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|█████▋    | 366/648 [11:33<08:43,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 367/648 [11:35<08:54,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 368/648 [11:37<08:52,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 369/648 [11:39<08:47,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 370/648 [11:41<08:56,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 371/648 [11:43<08:59,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 372/648 [11:45<08:55,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 373/648 [11:46<08:51,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 374/648 [11:48<08:55,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 375/648 [11:50<08:46,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 376/648 [11:52<08:34,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 377/648 [11:54<08:30,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 378/648 [11:56<08:25,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 379/648 [11:58<08:24,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▊    | 380/648 [12:00<08:20,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▉    | 381/648 [12:01<08:20,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.54 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▉    | 382/648 [12:03<08:21,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▉    | 383/648 [12:05<08:28,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▉    | 384/648 [12:07<08:25,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▉    | 385/648 [12:09<08:19,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|█████▉    | 386/648 [12:11<08:15,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|█████▉    | 387/648 [12:13<08:17,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|█████▉    | 388/648 [12:15<08:21,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 389/648 [12:17<08:16,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 390/648 [12:19<08:12,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 391/648 [12:21<08:16,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|██████    | 392/648 [12:23<08:08,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████    | 393/648 [12:24<08:03,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████    | 394/648 [12:26<08:02,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████    | 395/648 [12:28<07:54,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████    | 396/648 [12:30<07:53,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████▏   | 397/648 [12:32<07:50,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████▏   | 398/648 [12:34<07:50,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 399/648 [12:36<07:51,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 400/648 [12:38<07:51,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 401/648 [12:40<07:48,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 402/648 [12:41<07:42,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 403/648 [12:43<07:38,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▏   | 404/648 [12:45<07:39,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  62%|██████▎   | 405/648 [12:47<07:41,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 406/648 [12:49<07:43,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 407/648 [12:51<07:44,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 408/648 [12:53<07:43,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 409/648 [12:55<07:35,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 410/648 [12:57<07:31,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 411/648 [12:59<07:30,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▎   | 412/648 [13:00<07:29,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▎   | 413/648 [13:02<07:27,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▍   | 414/648 [13:04<07:24,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▍   | 415/648 [13:06<07:20,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▍   | 416/648 [13:08<07:26,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  64%|██████▍   | 417/648 [13:10<07:24,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▍   | 418/648 [13:12<07:19,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▍   | 419/648 [13:14<07:16,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▍   | 420/648 [13:16<07:21,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▍   | 421/648 [13:18<07:12,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▌   | 422/648 [13:20<07:08,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▌   | 423/648 [13:21<07:08,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▌   | 424/648 [13:23<07:08,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|██████▌   | 425/648 [13:25<07:10,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|██████▌   | 426/648 [13:27<07:05,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|██████▌   | 427/648 [13:29<06:59,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|██████▌   | 428/648 [13:31<06:54,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|██████▌   | 429/648 [13:33<06:53,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|██████▋   | 430/648 [13:35<06:50,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 431/648 [13:37<06:50,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 432/648 [13:39<06:45,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 433/648 [13:41<06:53,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 434/648 [13:42<06:48,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.70 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 435/648 [13:44<06:40,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 436/648 [13:46<06:37,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  67%|██████▋   | 437/648 [13:48<06:33,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 438/648 [13:50<06:30,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 439/648 [13:52<06:35,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 440/648 [13:54<06:36,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 441/648 [13:56<06:40,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 442/648 [13:58<06:41,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 443/648 [14:00<06:36,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▊   | 444/648 [14:01<06:29,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▊   | 445/648 [14:03<06:25,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▉   | 446/648 [14:05<06:21,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▉   | 447/648 [14:07<06:17,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▉   | 448/648 [14:09<06:12,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▉   | 449/648 [14:11<06:16,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  69%|██████▉   | 450/648 [14:13<06:22,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|██████▉   | 451/648 [14:15<06:19,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|██████▉   | 452/648 [14:17<06:17,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|██████▉   | 453/648 [14:19<06:11,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|███████   | 454/648 [14:20<06:08,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|███████   | 455/648 [14:22<06:03,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|███████   | 456/648 [14:24<06:01,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 457/648 [14:26<05:59,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 458/648 [14:28<06:02,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 459/648 [14:30<05:58,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 460/648 [14:32<05:55,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 461/648 [14:34<05:53,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████▏  | 462/648 [14:35<05:50,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████▏  | 463/648 [14:37<05:48,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 464/648 [14:39<05:45,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 465/648 [14:41<05:39,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 466/648 [14:43<05:40,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 467/648 [14:45<05:43,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 468/648 [14:47<05:40,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|███████▏  | 469/648 [14:49<05:36,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 470/648 [14:50<05:31,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 471/648 [14:52<05:33,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 472/648 [14:54<05:31,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 473/648 [14:56<05:25,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 474/648 [14:58<05:25,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 475/648 [15:00<05:31,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|███████▎  | 476/648 [15:02<05:33,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▎  | 477/648 [15:04<05:26,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▍  | 478/648 [15:06<05:21,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▍  | 479/648 [15:08<05:16,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▍  | 480/648 [15:09<05:15,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▍  | 481/648 [15:11<05:12,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▍  | 482/648 [15:13<05:10,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▍  | 483/648 [15:15<05:15,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▍  | 484/648 [15:17<05:20,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▍  | 485/648 [15:19<05:14,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 486/648 [15:21<05:10,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.64 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 487/648 [15:23<05:05,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 488/648 [15:25<05:01,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 489/648 [15:27<04:59,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|███████▌  | 490/648 [15:28<04:55,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|███████▌  | 491/648 [15:30<04:58,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|███████▌  | 492/648 [15:32<05:03,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.83 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|███████▌  | 493/648 [15:34<05:01,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|███████▌  | 494/648 [15:36<04:54,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|███████▋  | 495/648 [15:38<04:49,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 496/648 [15:40<04:53,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 497/648 [15:42<04:50,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 498/648 [15:44<04:48,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 499/648 [15:46<04:44,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 500/648 [15:48<04:48,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.78 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 501/648 [15:50<04:51,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.81 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 502/648 [15:52<04:51,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.61 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 503/648 [15:54<04:48,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 504/648 [15:56<04:45,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 505/648 [15:58<04:40,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 506/648 [16:00<04:39,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 507/648 [16:02<04:39,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.83 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  78%|███████▊  | 508/648 [16:04<04:42,  2.02s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.86 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▊  | 509/648 [16:06<04:42,  2.03s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▊  | 510/648 [16:08<04:33,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▉  | 511/648 [16:10<04:27,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▉  | 512/648 [16:12<04:27,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▉  | 513/648 [16:14<04:28,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▉  | 514/648 [16:16<04:30,  2.02s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▉  | 515/648 [16:18<04:26,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|███████▉  | 516/648 [16:20<04:24,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|███████▉  | 517/648 [16:22<04:26,  2.03s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|███████▉  | 518/648 [16:24<04:19,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████  | 519/648 [16:26<04:15,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████  | 520/648 [16:28<04:14,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  80%|████████  | 521/648 [16:30<04:15,  2.01s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 522/648 [16:32<04:11,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 523/648 [16:34<04:10,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 524/648 [16:36<04:07,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.88 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 525/648 [16:38<04:10,  2.04s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 526/648 [16:40<04:08,  2.04s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████▏ | 527/648 [16:42<04:00,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████▏ | 528/648 [16:44<03:55,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 529/648 [16:46<03:51,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 530/648 [16:47<03:45,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 531/648 [16:49<03:43,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 532/648 [16:51<03:43,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 533/648 [16:53<03:43,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  82%|████████▏ | 534/648 [16:55<03:41,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 535/648 [16:57<03:40,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.62 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 536/648 [16:59<03:37,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 537/648 [17:01<03:36,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 538/648 [17:03<03:34,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 539/648 [17:05<03:29,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 540/648 [17:07<03:27,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 541/648 [17:09<03:27,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▎ | 542/648 [17:11<03:26,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▍ | 543/648 [17:13<03:22,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▍ | 544/648 [17:15<03:20,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▍ | 545/648 [17:17<03:18,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▍ | 546/648 [17:18<03:16,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▍ | 547/648 [17:20<03:12,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▍ | 548/648 [17:22<03:10,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▍ | 549/648 [17:24<03:11,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▍ | 550/648 [17:26<03:09,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▌ | 551/648 [17:28<03:04,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▌ | 552/648 [17:30<03:02,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▌ | 553/648 [17:32<02:59,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▌ | 554/648 [17:34<02:56,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  86%|████████▌ | 555/648 [17:35<02:53,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  86%|████████▌ | 556/648 [17:37<02:52,  1.87s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  86%|████████▌ | 557/648 [17:39<02:51,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  86%|████████▌ | 558/648 [17:41<02:52,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  86%|████████▋ | 559/648 [17:43<02:49,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  86%|████████▋ | 560/648 [17:45<02:47,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 561/648 [17:47<02:44,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 562/648 [17:49<02:43,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 563/648 [17:51<02:41,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 564/648 [17:53<02:38,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 565/648 [17:54<02:37,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 566/648 [17:56<02:39,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.82 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 567/648 [17:59<02:39,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 568/648 [18:00<02:37,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 569/648 [18:02<02:35,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 570/648 [18:04<02:30,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 571/648 [18:06<02:27,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 572/648 [18:08<02:27,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  88%|████████▊ | 573/648 [18:10<02:24,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▊ | 574/648 [18:12<02:23,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▊ | 575/648 [18:14<02:22,  1.95s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▉ | 576/648 [18:16<02:21,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▉ | 577/648 [18:18<02:19,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▉ | 578/648 [18:20<02:17,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|████████▉ | 579/648 [18:22<02:16,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|████████▉ | 580/648 [18:24<02:13,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.83 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|████████▉ | 581/648 [18:26<02:11,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|████████▉ | 582/648 [18:28<02:09,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|████████▉ | 583/648 [18:30<02:08,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.86 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 584/648 [18:32<02:05,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 585/648 [18:34<02:03,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 586/648 [18:36<02:04,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████ | 587/648 [18:38<02:02,  2.01s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████ | 588/648 [18:40<01:57,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████ | 589/648 [18:42<01:53,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████ | 590/648 [18:43<01:51,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████ | 591/648 [18:45<01:50,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  91%|█████████▏| 592/648 [18:47<01:47,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 593/648 [18:49<01:44,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 594/648 [18:51<01:42,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 595/648 [18:53<01:40,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.75 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 596/648 [18:55<01:37,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 597/648 [18:57<01:36,  1.90s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 598/648 [18:59<01:36,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.76 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  92%|█████████▏| 599/648 [19:01<01:38,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.88 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 600/648 [19:03<01:36,  2.02s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 601/648 [19:05<01:36,  2.05s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 602/648 [19:07<01:32,  2.02s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 603/648 [19:09<01:30,  2.01s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 604/648 [19:11<01:27,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|█████████▎| 605/648 [19:13<01:24,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▎| 606/648 [19:15<01:22,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▎| 607/648 [19:17<01:21,  1.98s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▍| 608/648 [19:19<01:17,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▍| 609/648 [19:21<01:16,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▍| 610/648 [19:23<01:14,  1.96s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▍| 611/648 [19:25<01:11,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▍| 612/648 [19:26<01:09,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▍| 613/648 [19:28<01:06,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▍| 614/648 [19:30<01:04,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▍| 615/648 [19:32<01:03,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▌| 616/648 [19:34<01:01,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▌| 617/648 [19:36<00:58,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  95%|█████████▌| 618/648 [19:38<00:55,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▌| 619/648 [19:40<00:54,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▌| 620/648 [19:41<00:52,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▌| 621/648 [19:43<00:50,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▌| 622/648 [19:45<00:48,  1.85s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▌| 623/648 [19:47<00:46,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▋| 624/648 [19:49<00:45,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|█████████▋| 625/648 [19:51<00:42,  1.86s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 626/648 [19:53<00:41,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 627/648 [19:54<00:39,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 628/648 [19:56<00:38,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 629/648 [19:58<00:36,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 630/648 [20:00<00:34,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 631/648 [20:02<00:32,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 632/648 [20:04<00:30,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 633/648 [20:06<00:29,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 634/648 [20:08<00:26,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 635/648 [20:10<00:24,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 636/648 [20:12<00:22,  1.88s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 637/648 [20:14<00:20,  1.89s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 638/648 [20:16<00:19,  1.92s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.88 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▊| 639/648 [20:18<00:17,  1.93s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▉| 640/648 [20:19<00:15,  1.91s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▉| 641/648 [20:21<00:13,  1.94s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▉| 642/648 [20:24<00:11,  1.97s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▉| 643/648 [20:26<00:10,  2.01s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|█████████▉| 644/648 [20:28<00:08,  2.00s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 645/648 [20:30<00:05,  1.99s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 646/648 [20:32<00:04,  2.02s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████▉| 647/648 [20:34<00:02,  2.01s/it]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 648/648 [20:36<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    AutoModelForSequenceClassification.from_pretrained('checkpoints/checkpoint-10000'),\n",
    "    TrainingArguments(output_dir = './results', per_device_eval_batch_size = 2)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "start_time = time.time()\n",
    "ndgc5_combined_model = compute_analyzer_mean_ncdgn(trainer, tokenizer, multilingual_e5_retriever, 5)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4c2b526-4a44-40fd-89c8-b33c6345d620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NCDG@5 for combined model: 0.20061444082458438\n",
      "Time: 1236.1415512561798\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean NCDG@5 for combined model: {ndgc5_combined_model}\\nTime: {total_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600780d5-6a3e-4c77-a94d-dd4438545002",
   "metadata": {},
   "source": [
    "## Task 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a674c9-e817-4ae9-a4aa-34550f4723f1",
   "metadata": {},
   "source": [
    "**Bonus** (+2p) Use a different dense encoder, e.g. [E5 large](https://huggingface.co/intfloat/multilingual-e5-large) or [Polish Roberta Base](https://huggingface.co/sdadas/mmlw-retrieval-roberta-base) and compute NDCG@5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d850debc-8e4d-421e-9364-352b40eb4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "polish_roberta_retriever = set_retriever('polish_roberta', 'sdadas/mmlw-retrieval-roberta-base', fiqa_pl_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f915f7c-58ba-4526-b7c6-a16a43c4ef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/648 [00:00<?, ?it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:   0%|          | 1/648 [00:00<08:47,  1.23it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:   0%|          | 2/648 [00:01<06:36,  1.63it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:   0%|          | 3/648 [00:01<05:52,  1.83it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 4/648 [00:02<05:34,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 5/648 [00:02<05:24,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 6/648 [00:03<05:22,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 7/648 [00:03<05:16,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:   1%|          | 8/648 [00:04<05:14,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:   1%|▏         | 9/648 [00:04<05:13,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 10/648 [00:05<05:10,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 11/648 [00:05<05:03,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 12/648 [00:06<04:56,  2.14it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 13/648 [00:06<04:59,  2.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 14/648 [00:06<04:57,  2.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 15/648 [00:07<04:53,  2.16it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:   2%|▏         | 16/648 [00:07<04:54,  2.15it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 17/648 [00:08<04:53,  2.15it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 18/648 [00:08<04:52,  2.15it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 19/648 [00:09<04:54,  2.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 20/648 [00:09<04:56,  2.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 21/648 [00:10<04:53,  2.14it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:   3%|▎         | 22/648 [00:10<05:00,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:   4%|▎         | 23/648 [00:11<04:57,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:   4%|▎         | 24/648 [00:11<04:56,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 25/648 [00:12<04:59,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.86 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 26/648 [00:12<05:04,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 27/648 [00:13<05:08,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 28/648 [00:13<05:04,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:   4%|▍         | 29/648 [00:14<05:03,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:   5%|▍         | 30/648 [00:14<04:59,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:   5%|▍         | 31/648 [00:15<04:54,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:   5%|▍         | 32/648 [00:15<04:51,  2.11it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:   5%|▌         | 33/648 [00:16<04:46,  2.15it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:   5%|▌         | 34/648 [00:16<04:43,  2.16it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:   5%|▌         | 35/648 [00:16<04:45,  2.15it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 36/648 [00:17<04:41,  2.17it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 37/648 [00:17<04:42,  2.16it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 38/648 [00:18<04:47,  2.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 39/648 [00:18<04:53,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:   6%|▌         | 40/648 [00:19<04:58,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:   6%|▋         | 41/648 [00:19<04:54,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:   6%|▋         | 42/648 [00:20<04:52,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 43/648 [00:20<04:48,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 44/648 [00:21<04:53,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 45/648 [00:21<04:51,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 46/648 [00:22<04:43,  2.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 47/648 [00:22<04:42,  2.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\u001b[A\n",
      "Processing:   7%|▋         | 48/648 [00:23<04:35,  2.18it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 49/648 [00:23<04:35,  2.18it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 50/648 [00:24<04:36,  2.16it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 51/648 [00:24<04:41,  2.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 52/648 [00:24<04:35,  2.16it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 53/648 [00:25<04:33,  2.18it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 54/648 [00:25<04:39,  2.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:   8%|▊         | 55/648 [00:26<04:36,  2.14it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:   9%|▊         | 56/648 [00:26<04:35,  2.15it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 57/648 [00:27<04:32,  2.17it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 58/648 [00:27<04:35,  2.14it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 59/648 [00:28<04:36,  2.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 60/648 [00:28<04:39,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:   9%|▉         | 61/648 [00:29<04:46,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  10%|▉         | 62/648 [00:29<04:45,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  10%|▉         | 63/648 [00:30<04:38,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  10%|▉         | 64/648 [00:30<04:38,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  10%|█         | 65/648 [00:31<04:36,  2.11it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.73 Batches/s]\u001b[A\n",
      "Processing:  10%|█         | 66/648 [00:31<04:35,  2.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  10%|█         | 67/648 [00:32<04:40,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\u001b[A\n",
      "Processing:  10%|█         | 68/648 [00:32<04:37,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  11%|█         | 69/648 [00:33<04:35,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  11%|█         | 70/648 [00:33<04:35,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n",
      "Processing:  11%|█         | 71/648 [00:34<04:33,  2.11it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  11%|█         | 72/648 [00:34<04:35,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  11%|█▏        | 73/648 [00:34<04:35,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\u001b[A\n",
      "Processing:  11%|█▏        | 74/648 [00:35<04:33,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 75/648 [00:35<04:32,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.64 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 76/648 [00:36<04:28,  2.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 77/648 [00:36<04:29,  2.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 78/648 [00:37<04:32,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 79/648 [00:37<04:30,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n",
      "Processing:  12%|█▏        | 80/648 [00:38<04:29,  2.11it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  12%|█▎        | 81/648 [00:38<04:30,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 82/648 [00:39<04:30,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 83/648 [00:39<04:30,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 84/648 [00:40<04:31,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 85/648 [00:40<04:26,  2.11it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.65 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 86/648 [00:41<04:23,  2.13it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  13%|█▎        | 87/648 [00:41<04:25,  2.12it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  14%|█▎        | 88/648 [00:42<04:29,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  14%|█▎        | 89/648 [00:42<04:29,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  14%|█▍        | 90/648 [00:43<04:31,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  14%|█▍        | 91/648 [00:43<04:30,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  14%|█▍        | 92/648 [00:44<04:33,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  14%|█▍        | 93/648 [00:44<04:36,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  15%|█▍        | 94/648 [00:45<04:39,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  15%|█▍        | 95/648 [00:45<04:38,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  15%|█▍        | 96/648 [00:46<04:33,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  15%|█▍        | 97/648 [00:46<04:31,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.70 Batches/s]\u001b[A\n",
      "Processing:  15%|█▌        | 98/648 [00:47<04:24,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  15%|█▌        | 99/648 [00:47<04:24,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n",
      "Processing:  15%|█▌        | 100/648 [00:47<04:23,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.62 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 101/648 [00:48<04:19,  2.11it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 102/648 [00:48<04:20,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 103/648 [00:49<04:21,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 104/648 [00:49<04:21,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  16%|█▌        | 105/648 [00:50<04:21,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  16%|█▋        | 106/648 [00:50<04:21,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 107/648 [00:51<04:19,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 108/648 [00:51<04:20,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 109/648 [00:52<04:21,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 110/648 [00:52<04:17,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 111/648 [00:53<04:17,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 112/648 [00:53<04:26,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  17%|█▋        | 113/648 [00:54<04:31,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 114/648 [00:54<04:31,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 115/648 [00:55<04:31,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 116/648 [00:55<04:27,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 117/648 [00:56<04:28,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 118/648 [00:56<04:31,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  18%|█▊        | 119/648 [00:57<04:31,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  19%|█▊        | 120/648 [00:57<04:33,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  19%|█▊        | 121/648 [00:58<04:31,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 122/648 [00:58<04:34,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 123/648 [00:59<04:31,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 124/648 [01:00<04:36,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 125/648 [01:00<04:36,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  19%|█▉        | 126/648 [01:01<04:36,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.91 Batches/s]\u001b[A\n",
      "Processing:  20%|█▉        | 127/648 [01:01<04:37,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  20%|█▉        | 128/648 [01:02<04:39,  1.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  20%|█▉        | 129/648 [01:02<04:34,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  20%|██        | 130/648 [01:03<04:32,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  20%|██        | 131/648 [01:03<04:27,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  20%|██        | 132/648 [01:04<04:27,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 133/648 [01:04<04:22,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 134/648 [01:05<04:18,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 135/648 [01:05<04:18,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.69 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 136/648 [01:06<04:11,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  21%|██        | 137/648 [01:06<04:11,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.61 Batches/s]\u001b[A\n",
      "Processing:  21%|██▏       | 138/648 [01:07<04:07,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  21%|██▏       | 139/648 [01:07<04:07,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 140/648 [01:08<04:07,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 141/648 [01:08<04:06,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 142/648 [01:09<04:07,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 143/648 [01:09<04:10,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.62 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 144/648 [01:10<04:05,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  22%|██▏       | 145/648 [01:10<04:03,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 146/648 [01:11<04:02,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 147/648 [01:11<04:02,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 148/648 [01:12<04:05,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 149/648 [01:12<04:08,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 150/648 [01:13<04:05,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 151/648 [01:13<04:03,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n",
      "Processing:  23%|██▎       | 152/648 [01:13<04:00,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  24%|██▎       | 153/648 [01:14<04:01,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 154/648 [01:15<04:08,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 155/648 [01:15<04:04,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 156/648 [01:16<04:10,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 157/648 [01:16<04:11,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  24%|██▍       | 158/648 [01:17<04:11,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  25%|██▍       | 159/648 [01:17<04:12,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  25%|██▍       | 160/648 [01:18<04:06,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  25%|██▍       | 161/648 [01:18<04:05,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  25%|██▌       | 162/648 [01:19<04:03,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  25%|██▌       | 163/648 [01:19<03:59,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  25%|██▌       | 164/648 [01:20<04:00,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  25%|██▌       | 165/648 [01:20<04:01,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 166/648 [01:21<03:58,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 167/648 [01:21<03:55,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 168/648 [01:22<03:55,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 169/648 [01:22<03:58,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  26%|██▌       | 170/648 [01:23<04:00,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  26%|██▋       | 171/648 [01:23<03:59,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 172/648 [01:24<04:00,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 173/648 [01:24<04:03,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 174/648 [01:25<04:04,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.56 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 175/648 [01:25<03:56,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 176/648 [01:26<03:52,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 177/648 [01:26<03:49,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  27%|██▋       | 178/648 [01:27<03:54,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 179/648 [01:27<03:56,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 180/648 [01:28<03:55,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 181/648 [01:28<03:52,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 182/648 [01:29<03:58,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 183/648 [01:29<04:04,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  28%|██▊       | 184/648 [01:30<04:01,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  29%|██▊       | 185/648 [01:30<04:03,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  29%|██▊       | 186/648 [01:31<03:59,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 187/648 [01:31<03:52,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 188/648 [01:32<03:55,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 189/648 [01:32<03:57,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 190/648 [01:33<03:56,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  29%|██▉       | 191/648 [01:33<03:53,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  30%|██▉       | 192/648 [01:34<03:50,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  30%|██▉       | 193/648 [01:34<03:46,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n",
      "Processing:  30%|██▉       | 194/648 [01:35<03:44,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  30%|███       | 195/648 [01:35<03:46,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  30%|███       | 196/648 [01:36<03:43,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  30%|███       | 197/648 [01:36<03:44,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 198/648 [01:37<03:44,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.63 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 199/648 [01:37<03:38,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 200/648 [01:38<03:38,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 201/648 [01:38<03:37,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  31%|███       | 202/648 [01:39<03:35,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  31%|███▏      | 203/648 [01:39<03:40,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n",
      "Processing:  31%|███▏      | 204/648 [01:40<03:35,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 205/648 [01:40<03:33,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 206/648 [01:41<03:36,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 207/648 [01:41<03:36,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 208/648 [01:42<03:35,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 209/648 [01:42<03:35,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  32%|███▏      | 210/648 [01:43<03:37,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 211/648 [01:43<03:35,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 212/648 [01:44<03:32,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 213/648 [01:44<03:30,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 214/648 [01:44<03:29,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 215/648 [01:45<03:31,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.46 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 216/648 [01:45<03:28,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  33%|███▎      | 217/648 [01:46<03:29,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  34%|███▎      | 218/648 [01:46<03:27,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 219/648 [01:47<03:29,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 220/648 [01:47<03:32,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 221/648 [01:48<03:34,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 222/648 [01:48<03:35,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  34%|███▍      | 223/648 [01:49<03:35,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.74 Batches/s]\u001b[A\n",
      "Processing:  35%|███▍      | 224/648 [01:49<03:27,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  35%|███▍      | 225/648 [01:50<03:27,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\u001b[A\n",
      "Processing:  35%|███▍      | 226/648 [01:50<03:24,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n",
      "Processing:  35%|███▌      | 227/648 [01:51<03:22,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n",
      "Processing:  35%|███▌      | 228/648 [01:51<03:20,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  35%|███▌      | 229/648 [01:52<03:20,  2.09it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  35%|███▌      | 230/648 [01:52<03:23,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  36%|███▌      | 231/648 [01:53<03:24,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  36%|███▌      | 232/648 [01:53<03:27,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  36%|███▌      | 233/648 [01:54<03:29,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.61 Batches/s]\u001b[A\n",
      "Processing:  36%|███▌      | 234/648 [01:54<03:25,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n",
      "Processing:  36%|███▋      | 235/648 [01:55<03:24,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n",
      "Processing:  36%|███▋      | 236/648 [01:55<03:22,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 237/648 [01:56<03:22,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 238/648 [01:56<03:20,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 239/648 [01:57<03:20,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 240/648 [01:57<03:22,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.65 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 241/648 [01:58<03:16,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  37%|███▋      | 242/648 [01:58<03:18,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 243/648 [01:59<03:20,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 244/648 [01:59<03:19,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 245/648 [02:00<03:20,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 246/648 [02:00<03:20,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 247/648 [02:01<03:20,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 248/648 [02:01<03:17,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  38%|███▊      | 249/648 [02:02<03:19,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  39%|███▊      | 250/648 [02:02<03:26,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n",
      "Processing:  39%|███▊      | 251/648 [02:03<03:35,  1.84it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n",
      "Processing:  39%|███▉      | 252/648 [02:03<03:36,  1.83it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.82 Batches/s]\u001b[A\n",
      "Processing:  39%|███▉      | 253/648 [02:04<03:39,  1.80it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.91 Batches/s]\u001b[A\n",
      "Processing:  39%|███▉      | 254/648 [02:05<03:41,  1.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  39%|███▉      | 255/648 [02:05<03:38,  1.80it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  40%|███▉      | 256/648 [02:06<03:35,  1.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  40%|███▉      | 257/648 [02:06<03:30,  1.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  40%|███▉      | 258/648 [02:07<03:29,  1.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  40%|███▉      | 259/648 [02:07<03:23,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  40%|████      | 260/648 [02:08<03:17,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  40%|████      | 261/648 [02:08<03:15,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  40%|████      | 262/648 [02:09<03:14,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 263/648 [02:09<03:11,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 264/648 [02:10<03:10,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 265/648 [02:10<03:08,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 266/648 [02:11<03:06,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  41%|████      | 267/648 [02:11<03:07,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  41%|████▏     | 268/648 [02:12<03:08,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 269/648 [02:12<03:08,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 270/648 [02:13<03:10,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 271/648 [02:13<03:13,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 272/648 [02:14<03:11,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.91 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 273/648 [02:14<03:15,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 274/648 [02:15<03:17,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  42%|████▏     | 275/648 [02:15<03:12,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 276/648 [02:16<03:07,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 277/648 [02:16<03:04,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 278/648 [02:17<03:02,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 279/648 [02:17<02:59,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 280/648 [02:18<03:00,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  43%|████▎     | 281/648 [02:18<03:00,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  44%|████▎     | 282/648 [02:19<03:02,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  44%|████▎     | 283/648 [02:19<03:06,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.88 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 284/648 [02:20<03:12,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 285/648 [02:20<03:15,  1.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 286/648 [02:21<03:12,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 287/648 [02:21<03:13,  1.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  44%|████▍     | 288/648 [02:22<03:09,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  45%|████▍     | 289/648 [02:22<03:05,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  45%|████▍     | 290/648 [02:23<03:05,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n",
      "Processing:  45%|████▍     | 291/648 [02:23<03:01,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  45%|████▌     | 292/648 [02:24<02:59,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.51 Batches/s]\u001b[A\n",
      "Processing:  45%|████▌     | 293/648 [02:24<02:56,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  45%|████▌     | 294/648 [02:25<02:53,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 295/648 [02:25<02:55,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 296/648 [02:26<02:53,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 297/648 [02:26<02:52,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 298/648 [02:27<02:50,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.64 Batches/s]\u001b[A\n",
      "Processing:  46%|████▌     | 299/648 [02:27<02:49,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  46%|████▋     | 300/648 [02:28<02:52,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n",
      "Processing:  46%|████▋     | 301/648 [02:28<02:50,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.85 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 302/648 [02:29<02:46,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 303/648 [02:29<02:48,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 304/648 [02:30<02:47,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.66 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 305/648 [02:30<02:44,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 306/648 [02:31<02:44,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  47%|████▋     | 307/648 [02:31<02:45,  2.06it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 308/648 [02:32<02:43,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 309/648 [02:32<02:47,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.83 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 310/648 [02:33<02:42,  2.08it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 311/648 [02:33<02:46,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 312/648 [02:34<02:47,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 313/648 [02:34<02:47,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  48%|████▊     | 314/648 [02:35<02:46,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  49%|████▊     | 315/648 [02:35<02:46,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 316/648 [02:36<02:48,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 317/648 [02:36<02:49,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 318/648 [02:37<02:53,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 319/648 [02:37<02:49,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  49%|████▉     | 320/648 [02:38<02:46,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  50%|████▉     | 321/648 [02:38<02:48,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n",
      "Processing:  50%|████▉     | 322/648 [02:39<02:45,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\u001b[A\n",
      "Processing:  50%|████▉     | 323/648 [02:39<02:39,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  50%|█████     | 324/648 [02:40<02:40,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n",
      "Processing:  50%|█████     | 325/648 [02:40<02:38,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  50%|█████     | 326/648 [02:41<02:37,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  50%|█████     | 327/648 [02:41<02:36,  2.05it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 328/648 [02:42<02:36,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 329/648 [02:42<02:38,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 330/648 [02:43<02:37,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 331/648 [02:43<02:35,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  51%|█████     | 332/648 [02:44<02:33,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  51%|█████▏    | 333/648 [02:44<02:35,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 334/648 [02:45<02:35,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 335/648 [02:45<02:33,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 336/648 [02:46<02:34,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 337/648 [02:46<02:33,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.42 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 338/648 [02:47<02:33,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 339/648 [02:47<02:33,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.71 Batches/s]\u001b[A\n",
      "Processing:  52%|█████▏    | 340/648 [02:48<02:28,  2.07it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.60 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 341/648 [02:48<02:26,  2.10it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 342/648 [02:49<02:31,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 343/648 [02:49<02:29,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 344/648 [02:50<02:29,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 345/648 [02:50<02:29,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  53%|█████▎    | 346/648 [02:51<02:29,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▎    | 347/648 [02:51<02:29,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▎    | 348/648 [02:52<02:30,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 349/648 [02:52<02:33,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 350/648 [02:53<02:32,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 351/648 [02:53<02:33,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 352/648 [02:54<02:31,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  54%|█████▍    | 353/648 [02:54<02:31,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▍    | 354/648 [02:55<02:27,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▍    | 355/648 [02:55<02:25,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▍    | 356/648 [02:56<02:25,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▌    | 357/648 [02:56<02:26,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▌    | 358/648 [02:57<02:24,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  55%|█████▌    | 359/648 [02:57<02:27,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 360/648 [02:58<02:26,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 361/648 [02:58<02:26,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 362/648 [02:59<02:24,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 363/648 [02:59<02:23,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▌    | 364/648 [03:00<02:24,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.73 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▋    | 365/648 [03:00<02:20,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:  56%|█████▋    | 366/648 [03:01<02:22,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 367/648 [03:01<02:21,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 368/648 [03:02<02:18,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.82 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 369/648 [03:02<02:21,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.91 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 370/648 [03:03<02:23,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 371/648 [03:03<02:26,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  57%|█████▋    | 372/648 [03:04<02:25,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 373/648 [03:04<02:24,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 374/648 [03:05<02:19,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 375/648 [03:05<02:22,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 376/648 [03:06<02:21,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 377/648 [03:06<02:20,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 378/648 [03:07<02:18,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  58%|█████▊    | 379/648 [03:07<02:18,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▊    | 380/648 [03:08<02:18,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 381/648 [03:09<02:19,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.80 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 382/648 [03:09<02:21,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 383/648 [03:10<02:18,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 384/648 [03:10<02:15,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  59%|█████▉    | 385/648 [03:11<02:12,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  60%|█████▉    | 386/648 [03:11<02:14,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  60%|█████▉    | 387/648 [03:12<02:13,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  60%|█████▉    | 388/648 [03:12<02:11,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  60%|██████    | 389/648 [03:13<02:09,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  60%|██████    | 390/648 [03:13<02:10,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  60%|██████    | 391/648 [03:14<02:10,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  60%|██████    | 392/648 [03:14<02:07,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  61%|██████    | 393/648 [03:15<02:08,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:  61%|██████    | 394/648 [03:15<02:10,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  61%|██████    | 395/648 [03:16<02:09,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.83 Batches/s]\u001b[A\n",
      "Processing:  61%|██████    | 396/648 [03:16<02:10,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  61%|██████▏   | 397/648 [03:17<02:09,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  61%|██████▏   | 398/648 [03:17<02:07,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 399/648 [03:18<02:04,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.63 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 400/648 [03:18<02:02,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 401/648 [03:19<02:02,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 402/648 [03:19<02:01,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 403/648 [03:20<02:00,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▏   | 404/648 [03:20<02:02,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  62%|██████▎   | 405/648 [03:21<01:59,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 406/648 [03:21<01:59,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 407/648 [03:22<01:58,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 408/648 [03:22<01:59,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 409/648 [03:23<01:59,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 410/648 [03:23<01:57,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  63%|██████▎   | 411/648 [03:24<02:00,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▎   | 412/648 [03:24<02:00,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▎   | 413/648 [03:25<02:02,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▍   | 414/648 [03:25<02:01,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▍   | 415/648 [03:26<01:59,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▍   | 416/648 [03:26<01:57,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  64%|██████▍   | 417/648 [03:27<01:56,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▍   | 418/648 [03:27<01:56,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▍   | 419/648 [03:28<01:56,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.48 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▍   | 420/648 [03:28<01:54,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▍   | 421/648 [03:29<01:52,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▌   | 422/648 [03:29<01:53,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▌   | 423/648 [03:30<01:53,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  65%|██████▌   | 424/648 [03:30<01:51,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 425/648 [03:31<01:50,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 426/648 [03:31<01:49,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.67 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 427/648 [03:32<01:48,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 428/648 [03:32<01:47,  2.04it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▌   | 429/648 [03:33<01:48,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\u001b[A\n",
      "Processing:  66%|██████▋   | 430/648 [03:33<01:47,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 431/648 [03:34<01:47,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 432/648 [03:34<01:47,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.37 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 433/648 [03:35<01:46,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 434/648 [03:35<01:47,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 435/648 [03:36<01:45,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 436/648 [03:36<01:46,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  67%|██████▋   | 437/648 [03:37<01:46,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.63 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 438/648 [03:37<01:45,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 439/648 [03:38<01:44,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 440/648 [03:38<01:44,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 441/648 [03:39<01:44,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.32 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 442/648 [03:39<01:44,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  68%|██████▊   | 443/648 [03:40<01:46,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▊   | 444/648 [03:40<01:47,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▊   | 445/648 [03:41<01:49,  1.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 446/648 [03:41<01:47,  1.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 447/648 [03:42<01:46,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 448/648 [03:42<01:45,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.51 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 449/648 [03:43<01:42,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  69%|██████▉   | 450/648 [03:43<01:40,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  70%|██████▉   | 451/648 [03:44<01:38,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  70%|██████▉   | 452/648 [03:44<01:38,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  70%|██████▉   | 453/648 [03:45<01:38,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  70%|███████   | 454/648 [03:45<01:36,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.39 Batches/s]\u001b[A\n",
      "Processing:  70%|███████   | 455/648 [03:46<01:35,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  70%|███████   | 456/648 [03:46<01:35,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 457/648 [03:47<01:35,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 458/648 [03:47<01:35,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 459/648 [03:48<01:33,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 460/648 [03:48<01:33,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  71%|███████   | 461/648 [03:49<01:33,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\u001b[A\n",
      "Processing:  71%|███████▏  | 462/648 [03:49<01:32,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  71%|███████▏  | 463/648 [03:50<01:32,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 464/648 [03:50<01:32,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 465/648 [03:51<01:31,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 466/648 [03:51<01:31,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 467/648 [03:52<01:30,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 468/648 [03:52<01:30,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  72%|███████▏  | 469/648 [03:53<01:29,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 470/648 [03:53<01:29,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 471/648 [03:54<01:28,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 472/648 [03:54<01:28,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 473/648 [03:55<01:27,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 474/648 [03:55<01:27,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 475/648 [03:56<01:28,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  73%|███████▎  | 476/648 [03:56<01:29,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▎  | 477/648 [03:57<01:29,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 478/648 [03:58<01:29,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 479/648 [03:58<01:27,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 480/648 [03:59<01:27,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 481/648 [03:59<01:25,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  74%|███████▍  | 482/648 [04:00<01:25,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▍  | 483/648 [04:00<01:23,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▍  | 484/648 [04:01<01:23,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▍  | 485/648 [04:01<01:21,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▌  | 486/648 [04:02<01:22,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▌  | 487/648 [04:02<01:22,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▌  | 488/648 [04:03<01:22,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  75%|███████▌  | 489/648 [04:03<01:22,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 490/648 [04:04<01:23,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 491/648 [04:04<01:22,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 492/648 [04:05<01:22,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 493/648 [04:05<01:20,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▌  | 494/648 [04:06<01:19,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n",
      "Processing:  76%|███████▋  | 495/648 [04:06<01:17,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 496/648 [04:07<01:16,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 497/648 [04:07<01:15,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 498/648 [04:08<01:15,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 499/648 [04:08<01:14,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 500/648 [04:09<01:15,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 501/648 [04:09<01:14,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\u001b[A\n",
      "Processing:  77%|███████▋  | 502/648 [04:10<01:12,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 503/648 [04:10<01:12,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 504/648 [04:11<01:11,  2.03it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 505/648 [04:11<01:10,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 506/648 [04:12<01:11,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 507/648 [04:12<01:11,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n",
      "Processing:  78%|███████▊  | 508/648 [04:13<01:12,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▊  | 509/648 [04:13<01:12,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▊  | 510/648 [04:14<01:11,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 511/648 [04:14<01:09,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 512/648 [04:15<01:09,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 513/648 [04:15<01:08,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 514/648 [04:16<01:07,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  79%|███████▉  | 515/648 [04:16<01:06,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  80%|███████▉  | 516/648 [04:17<01:06,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  80%|███████▉  | 517/648 [04:17<01:05,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  80%|███████▉  | 518/648 [04:18<01:05,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  80%|████████  | 519/648 [04:18<01:05,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  80%|████████  | 520/648 [04:19<01:04,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  80%|████████  | 521/648 [04:19<01:03,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 522/648 [04:20<01:04,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 523/648 [04:20<01:04,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 524/648 [04:21<01:04,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 525/648 [04:21<01:02,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  81%|████████  | 526/648 [04:22<01:01,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  81%|████████▏ | 527/648 [04:22<01:01,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  81%|████████▏ | 528/648 [04:23<01:01,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 529/648 [04:23<01:00,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 530/648 [04:24<01:01,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 531/648 [04:24<00:59,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 532/648 [04:25<00:59,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 533/648 [04:25<00:58,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\u001b[A\n",
      "Processing:  82%|████████▏ | 534/648 [04:26<00:59,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 535/648 [04:27<00:58,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 536/648 [04:27<00:59,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.79 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 537/648 [04:28<00:59,  1.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.99 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 538/648 [04:28<00:59,  1.83it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.76 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 539/648 [04:29<01:00,  1.80it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 540/648 [04:29<00:59,  1.81it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  83%|████████▎ | 541/648 [04:30<00:57,  1.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▎ | 542/648 [04:30<00:55,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 543/648 [04:31<00:55,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 544/648 [04:31<00:53,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 545/648 [04:32<00:52,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 546/648 [04:32<00:51,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n",
      "Processing:  84%|████████▍ | 547/648 [04:33<00:51,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.47 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▍ | 548/648 [04:33<00:51,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▍ | 549/648 [04:34<00:49,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▍ | 550/648 [04:34<00:48,  2.01it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▌ | 551/648 [04:35<00:47,  2.02it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▌ | 552/648 [04:35<00:48,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▌ | 553/648 [04:36<00:48,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.50 Batches/s]\u001b[A\n",
      "Processing:  85%|████████▌ | 554/648 [04:36<00:46,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▌ | 555/648 [04:37<00:47,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▌ | 556/648 [04:37<00:46,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▌ | 557/648 [04:38<00:45,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▌ | 558/648 [04:38<00:45,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▋ | 559/648 [04:39<00:45,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.58 Batches/s]\u001b[A\n",
      "Processing:  86%|████████▋ | 560/648 [04:39<00:44,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 561/648 [04:40<00:43,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.87 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 562/648 [04:40<00:44,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 563/648 [04:41<00:44,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 564/648 [04:42<00:44,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 565/648 [04:42<00:43,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  87%|████████▋ | 566/648 [04:43<00:43,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.79 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 567/648 [04:43<00:44,  1.84it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.85 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 568/648 [04:44<00:44,  1.81it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.74 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 569/648 [04:44<00:44,  1.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.75 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 570/648 [04:45<00:44,  1.76it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.74 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 571/648 [04:46<00:44,  1.75it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.95 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 572/648 [04:46<00:42,  1.77it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  88%|████████▊ | 573/648 [04:47<00:42,  1.78it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.00 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▊ | 574/648 [04:47<00:41,  1.79it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▊ | 575/648 [04:48<00:40,  1.82it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▉ | 576/648 [04:48<00:38,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▉ | 577/648 [04:49<00:37,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.30 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▉ | 578/648 [04:49<00:36,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  89%|████████▉ | 579/648 [04:50<00:35,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  90%|████████▉ | 580/648 [04:50<00:34,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  90%|████████▉ | 581/648 [04:51<00:33,  1.99it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing:  90%|████████▉ | 582/648 [04:51<00:33,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.36 Batches/s]\u001b[A\n",
      "Processing:  90%|████████▉ | 583/648 [04:52<00:32,  2.00it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  90%|█████████ | 584/648 [04:52<00:32,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  90%|█████████ | 585/648 [04:53<00:32,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  90%|█████████ | 586/648 [04:53<00:32,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 587/648 [04:54<00:31,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.19 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 588/648 [04:54<00:30,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.29 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 589/648 [04:55<00:29,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 590/648 [04:55<00:29,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.31 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████ | 591/648 [04:56<00:28,  1.98it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  91%|█████████▏| 592/648 [04:56<00:28,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.08 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 593/648 [04:57<00:27,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.27 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 594/648 [04:57<00:27,  1.96it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 595/648 [04:58<00:26,  1.97it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 596/648 [04:58<00:26,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 597/648 [04:59<00:26,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 598/648 [04:59<00:25,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  92%|█████████▏| 599/648 [05:00<00:25,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 600/648 [05:00<00:25,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 601/648 [05:01<00:25,  1.84it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.75 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 602/648 [05:02<00:25,  1.80it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.05 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 603/648 [05:02<00:24,  1.81it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 604/648 [05:03<00:23,  1.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  93%|█████████▎| 605/648 [05:03<00:23,  1.86it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▎| 606/648 [05:04<00:22,  1.84it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.25 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▎| 607/648 [05:04<00:21,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.24 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 608/648 [05:05<00:20,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 609/648 [05:05<00:20,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 610/648 [05:06<00:19,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.26 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 611/648 [05:06<00:19,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.73 Batches/s]\u001b[A\n",
      "Processing:  94%|█████████▍| 612/648 [05:07<00:19,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▍| 613/648 [05:07<00:18,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.91 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▍| 614/648 [05:08<00:18,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.80 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▍| 615/648 [05:09<00:17,  1.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▌| 616/648 [05:09<00:17,  1.87it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▌| 617/648 [05:10<00:16,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  95%|█████████▌| 618/648 [05:10<00:15,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 619/648 [05:11<00:15,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 620/648 [05:11<00:14,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 621/648 [05:12<00:14,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 622/648 [05:12<00:13,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.21 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▌| 623/648 [05:13<00:12,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▋| 624/648 [05:13<00:12,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  96%|█████████▋| 625/648 [05:14<00:11,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.15 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 626/648 [05:14<00:11,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 627/648 [05:15<00:10,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 628/648 [05:15<00:10,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 629/648 [05:16<00:10,  1.88it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 630/648 [05:16<00:09,  1.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.94 Batches/s]\u001b[A\n",
      "Processing:  97%|█████████▋| 631/648 [05:17<00:09,  1.84it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.98 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 632/648 [05:17<00:08,  1.85it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.28 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 633/648 [05:18<00:07,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 634/648 [05:19<00:07,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 635/648 [05:19<00:06,  1.89it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 636/648 [05:20<00:06,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 637/648 [05:20<00:05,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.10 Batches/s]\u001b[A\n",
      "Processing:  98%|█████████▊| 638/648 [05:21<00:05,  1.90it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▊| 639/648 [05:21<00:04,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.07 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 640/648 [05:22<00:04,  1.91it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 641/648 [05:22<00:03,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 642/648 [05:23<00:03,  1.95it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.03 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 643/648 [05:23<00:02,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing:  99%|█████████▉| 644/648 [05:24<00:02,  1.94it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.11 Batches/s]\u001b[A\n",
      "Processing: 100%|█████████▉| 645/648 [05:24<00:01,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.20 Batches/s]\u001b[A\n",
      "Processing: 100%|█████████▉| 646/648 [05:25<00:01,  1.93it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\u001b[A\n",
      "Processing: 100%|█████████▉| 647/648 [05:25<00:00,  1.92it/s]\n",
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.13 Batches/s]\u001b[A\n",
      "Processing: 100%|██████████| 648/648 [05:26<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ndgc5_polish_roberta_retriever = compute_analyzer_mean_ncdgn(polish_roberta_retriever, 5)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13dca8b4-8079-4206-911a-e080d16d8086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NCDG@5 for dense encoder Polish Roberta Base: 0.15930412015496273\n",
      "Time: 326.26830315589905\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean NCDG@5 for dense encoder Polish Roberta Base: {ndgc5_polish_roberta_retriever}\\nTime: {total_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc52368-7b72-4789-8fed-909fab3c72c2",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d6da3-c37b-488f-b8f1-46f322d8da97",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd0b86e-8a3f-4082-a8f2-229ad4ed8fdc",
   "metadata": {},
   "source": [
    "Which of the methods: lexical match (e.g. ElasticSearch) or dense representation works better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbee479-99ae-4e36-b681-46d93c4bb67c",
   "metadata": {},
   "source": [
    "The obtained results do not clearly demonstrate a significant advantage of dense representation models over lexical matching (e.g., Elasticsearch). The highest NDCG@5 score was achieved by the multilingual E5 model (approximately 0.2347), which was higher than the score obtained for Elasticsearch in Lab 5 (approximately 0.2070). On the other hand, the NDCG@5 score for the dense encoder Polish Roberta Base was lower (approximately 0.1593). I assume that the lower NDCG@5 score for RoBERTa Base is due to the model's reliance on specific types of queries. Dense representations have the potential to outperform lexical matching, but their effectiveness depends on the model, data, and query type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb6f48-1b82-4e53-b827-7c20888079bd",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bef91d-5dfa-490d-bcfa-b98026ad044f",
   "metadata": {},
   "source": [
    "Which of the methods is faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd441e-1cad-43ae-878f-5444ee97b4a3",
   "metadata": {},
   "source": [
    "The obtained results indicate that lexical methods like Elasticsearch were significantly faster than dense representations. Saving and processing data in dense representations took a very long time. In contrast, reading data into Elasticsearch took approximately 2 minutes. However, it should be noted that the time performance of dense representations depends on the size of the language model used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a453d3-708a-49e1-8fb4-235cd36719e7",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d830ac1-6dd6-47d9-85d8-82d6355c8eb3",
   "metadata": {},
   "source": [
    "Try to determine the other pros and cons of using lexical search and dense document retrieval models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc54df-c63c-4d22-8043-963db6ea201c",
   "metadata": {},
   "source": [
    "In my opinion, both methods have as many advantages as disadvantages. Lexical search is said to be highly precise, when users provide queries with exact keywords. However, lexical search is limited in understanding the context and meaning of words within documents. It usually fails to capture lexically different terms. As a result, lexical search struggles with more complex queries where the exact terms used in the document don’t match the words in the query.\r\n",
    "\r\n",
    "In contrast, dense models are able to handle synonyms and capture similar concepts. Compared to lexical search, dense models perform better when queries are more complex or involve related concepts. These features enable dense models to find documents even when using complex, non-obvious queries. However, these advantages come with a price. Dense models have a high computational cost, especially when used with large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
