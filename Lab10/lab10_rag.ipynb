{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1b5930-e140-4360-8a5b-41c5d2f9e27b",
   "metadata": {},
   "source": [
    "# NLP Lab10 - RAG-based Question Answering\n",
    "\n",
    "**Author: Bartłomiej Jamiołkowski**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116284bf-5237-47ed-b131-9e201a4a5dee",
   "metadata": {},
   "source": [
    "The exercise introduces modern approaches to Question Answering using Retrieval Augmented Generation (RAG) with LLMs and vector databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ed78de-3416-4870-aee7-6c10282e4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from uuid import uuid4\n",
    "\n",
    "import chromadb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20432c4f-7852-41d6-b9fb-1812b4296eaf",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659b3a9-5546-4101-ad7e-788c4cc7c14c",
   "metadata": {},
   "source": [
    "Set up the QA environment:\r\n",
    "   * Install OLLAMA and select an appropriate LLM\r\n",
    "   * Configure [Qdrant](https://qdrant.tech/) vector database (or vector DB of your choosing)\r\n",
    "   * Install necessary Python packages for embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457b9874-fcb4-4425-ad64-f81c2bc43e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama(base_url = 'http://localhost:11434', model = 'gemma2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a7e0b-d87e-4c73-856e-31fd602ccc80",
   "metadata": {},
   "source": [
    "I decided to use Chroma vector database, which is configured in the following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913e4f6-6622-4148-af4b-1412267c1320",
   "metadata": {},
   "source": [
    "## Tasks 2 - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28e1fc-877b-424b-8698-cec2fc299bc5",
   "metadata": {},
   "source": [
    "Find PDF file of your choosing. Example - some publication or CV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80e0d7-eafd-4ec6-ad91-06a7f724f21a",
   "metadata": {},
   "source": [
    "Write next procedures necessary for RAG pipeline. Use [LangChain](https://python.langchain.com/docs/introduction/) library:\r\n",
    " \r\n",
    "   * Load PDF file using `PyPDFLoader`.  \r\n",
    "   * Split documents into appropriate chunks using `RecursiveCharacterTextSplitter`.\r\n",
    "   * Generate and store embeddings in Qdrant database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834db45-f38e-4637-8093-271d597329cd",
   "metadata": {},
   "source": [
    "For this task, I decided to use fictional CV in PDF format provided by [BeamJobs](https://www.beamjobs.com/resumes/nlp-data-scientist-resume-examples). Mentioned CV has only one page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fd93c1-7665-4fd5-8a95-e588f222546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path = './data/nlp-data-scientist-official-resume-example.pdf')\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036b51c5-6099-4556-a5b2-06b6093e3459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f426798-3925-4a4b-8177-585c75707d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAHUL MALIK\n",
      "NLP DATA SCIENTIST\n",
      "CONTACT\n",
      "rahulmalik@email.com\n",
      "(123) 456-7890\n",
      "Brooklyn, NY\n",
      "LinkedIn\n",
      "Git\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7618c69f-a61d-4e18-8641-5ff394f86f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap = 40, length_function = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe26286-1307-4f66-80f9-30804aa81c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: [Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='RAHUL MALIK\\nNLP DATA SCIENTIST\\nCONTACT\\nrahulmalik@email.com\\n(123) 456-7890\\nBrooklyn, NY\\nLinkedIn'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='(123) 456-7890\\nBrooklyn, NY\\nLinkedIn\\nGithub\\nEDUCATION\\nPhD\\nNatural Language\\nProcessing (NLP)'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='PhD\\nNatural Language\\nProcessing (NLP)\\nUniversity of Maryland\\nSeptember 2010 - April 2016'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='September 2010 - April 2016\\nCollege Park, MD\\nB.S.\\nStatistics\\nPrinceton University'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='B.S.\\nStatistics\\nPrinceton University\\nSeptember 2006 - April 2010\\nPrinceton, NJ\\nSKILLS'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Princeton, NJ\\nSKILLS\\nPython (NumPy, Pandas,\\nScikit-learn, Keras, Flask)\\nSQL (MySQL, Postgres)\\nGit'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='SQL (MySQL, Postgres)\\nGit\\nTime Series Forecasting\\nProductionizing Models\\nRecommendation Engines'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Recommendation Engines\\nCustomer Segmentation\\nAWS\\nNLP\\nWORK EXPERIENCE\\nNLP Data Scientist\\nAmazon'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='NLP Data Scientist\\nAmazon\\nMay 2018 - current / New York, NY'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='May 2018 - current / New York, NY\\nDeconstructed item descriptions in the \"home care\" category to'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='predict which features of a given product were most likely to be'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='relevant to a given customer, increasing conversions by 4%'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Built an automated system to predict whether a given review was'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='likely to be from a real user, leading to a reduction in \"fake\" reviews by\\n19%'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='19%\\nAnalyzed the quality of customer service responses for worst'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='performing vendors to help reduce their return rates by 5%'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Mentored 7 junior data scientists over 3 intern programs\\nData Scientist\\nPriceline'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Data Scientist\\nPriceline\\nApril 2016 - May 2018 / New York, NY'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Built a price sensitivity model to offer lower pricing for room'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='inventory unlikely to be booked, resulting in a decrease in room\\nvacancy of 17%'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='vacancy of 17%\\nPerformed sentiment analysis to reviews likely to be relevant to a'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='given user for a given room to increase booking by 6%'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Worked alongside product managers to construct queries to identify'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='customers who abandoned their checkout, leading to an email'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='sequence that improved conversion rate by 12%'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Streamlined feature selection for model to predict likelihood of a'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='customer to re-book on Priceline, which saved about 21 hours of\\nmanual work each month'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='manual work each month\\nData Scientist Internship\\nMicrosoft\\nApril 2015 - April 2016 / New York, NY'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='April 2015 - April 2016 / New York, NY\\nAnalyzed anonymous employee performance reviews to identify'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='regular areas for improvement for engineers leading to actionable\\nfeedback for over 200 engineers'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='feedback for over 200 engineers\\nWorked with the customer success team to understand feedback on'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Azure products for small businesses to improve on-boarding and'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='increase customer adoption rate by 14%'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='Built a model to predict whether a given customer was satisﬁed with'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='their customer success experience, resulting in improved CS coaching'), Document(metadata={'source': './data/nlp-data-scientist-official-resume-example.pdf', 'page': 0}, page_content='and 26% fewer customer complaints')]\n",
      "\n",
      "Number of chunks: 36\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.split_documents(pages)\n",
    "print(f'Chunks: {chunks}\\n\\nNumber of chunks: {len(chunks)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a472f81-5df6-4439-8425-8e92127703d8",
   "metadata": {},
   "source": [
    "The following code generates embeddings using the Ollama model 'gemma2' and stores them in the Chroma vector database, as specified in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3f3920-0e9a-42e2-89ba-5e30d930604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model = 'gemma2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd12bd4-a9a0-4ae2-9873-83835e9f3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name = 'lab10_rag_collection',\n",
    "    embedding_function = embeddings,\n",
    "    persist_directory = './chroma_langchain_db'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa10935-9f4d-42ed-bff5-94bcf2b912fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['247ab915-6c12-4dc4-9783-df5a128919a6',\n",
       " '5871e483-cf8b-4286-a861-a84cd9516885',\n",
       " 'b34d131e-62e3-4c29-a0fd-3f0ce9f3823a',\n",
       " 'c6029e14-6210-498f-9dc9-fd493ea4eb6b',\n",
       " '433ddd5e-ba4d-4a63-a125-482abdaebef3',\n",
       " '6b2bf3da-0ab0-44b7-acd7-49195a742764',\n",
       " '3d218436-0f1e-417a-af0a-9601a1168b97',\n",
       " '0a2b2507-9450-4061-bcb9-5d9f2dcfb3df',\n",
       " 'b475b447-3aa3-4ef9-82d2-f2385e4a98c6',\n",
       " 'd54711e2-7f6e-4568-8da2-afc6c938eaa7',\n",
       " '96e0c6dd-757c-4c36-a09e-0421aaf86a21',\n",
       " '6140ebfc-aeb6-4aed-953b-45389a85a4b8',\n",
       " '9a966c78-511a-44fe-b35b-a0489ab189ff',\n",
       " 'eeeea5fd-5db1-460e-84df-df5ccfb32657',\n",
       " '97ad0478-e33d-4b8f-a705-9e237294247a',\n",
       " '84a195a8-7a8b-4b5a-b44e-9975d2eb9049',\n",
       " 'f81149c3-4c50-4c0e-bb9a-09787f4fce4e',\n",
       " '2c96a75c-bded-4e71-b943-6e8e7b70808d',\n",
       " '953381cb-f5c5-4f8e-bcc5-be2be2357f70',\n",
       " 'be9fde12-ba2c-4f54-bf7a-76ad3d0d3916',\n",
       " '64eea69c-1357-4641-bc68-0e44d12bc0ed',\n",
       " '5d6ae52b-11a9-46aa-81ef-7996474650ca',\n",
       " '6b331817-6cb7-4c38-b1ab-4403ac2448dc',\n",
       " '59189352-538e-4b89-b07c-5ddabebc902c',\n",
       " '43b1fc3e-762d-4567-be6c-efaef2bf6270',\n",
       " 'b527dec1-1009-404f-b044-66317f3dfca9',\n",
       " '7cf5f7e8-8f3e-4f4b-92e0-1caf2c343cad',\n",
       " 'd7ae2462-b2d6-47a9-9336-4f84e365c1d2',\n",
       " '33e0a9c9-0293-4b2b-a73d-9bfa4336bff2',\n",
       " 'fa91e985-52de-43c0-b6af-861414571dc5',\n",
       " 'c7cf96e1-4d63-43a4-abdb-b1a7b1dbdb92',\n",
       " '7e3cfd94-bab3-40f3-a2a5-060888e7b16f',\n",
       " 'a571525e-c48a-40da-8a7a-2f874a42e5e2',\n",
       " '445fff9a-1269-46a7-9ad3-1c34e4dba3fa',\n",
       " '983149d0-2e50-439d-89d3-0745bbd0c68e',\n",
       " '1288f3a9-53fe-42e3-b5dc-77b3148a9963']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "vector_store.add_documents(documents = chunks, ids = uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d49318-1041-4cc8-8295-14ebd2463ebb",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189ae65-c007-4d6f-9626-fa3ccbe1d4a1",
   "metadata": {},
   "source": [
    "Design and implement the RAG pipeline with `LCEL`. As reference use this detailed guide created by LangChain community - [RAG](https://python.langchain.com/docs/tutorials/rag/). Next steps should involve:\r\n",
    "   * Create query embedding generation\r\n",
    "   * Implement semantic search in Qdrant\r\n",
    "   * Design prompt templates for context integration\r\n",
    "   * Build response generation with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193f934-c58c-4ee6-85b9-efb2042dce4f",
   "metadata": {},
   "source": [
    "In this task, I use a retriever that generates query embeddings and performs semantic search in Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd9879e3-fb95-4adf-ad70-6eaf28d17350",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs = {'k': 7})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010da6d-2cc3-4212-8db1-bfa045bd6477",
   "metadata": {},
   "source": [
    "Creating basic QA prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a404dc-97d6-4ae8-9744-5c9591308c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = ['context', 'query'],\n",
    "    template = '''\n",
    "    <start_of_turn>user\n",
    "    You are an AI assistant. Use the context below to answer the query. You are not allowed to write additional comments.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Query: {query}\n",
    "    <end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de358f7c-1b0a-4a14-ae95-65dc29bbf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_string(chunks):\n",
    "    return '\\n\\n'.join(chunk.page_content for chunk in chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fd707-f19d-44fe-ae00-1029e50f7c01",
   "metadata": {},
   "source": [
    "Implementing the RAG pipeline with LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d47bfa-79ee-4104-a050-ad26837d4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {'context': retriever | chunks_to_string, 'query': RunnablePassthrough()} | prompt | ollama | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcc994-1dd5-47f8-8b07-42d67cfd5864",
   "metadata": {},
   "source": [
    "I determine 5 evaluation queries for evaluation and further comparison purposes.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8a90186-8635-49a2-95e1-875c9ee8121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'What skills does Rahul Malik have?',\n",
    "    'Where does Rahul Malik live?',\n",
    "    'What is the current position of Rahul Malik?',\n",
    "    'What are the former employers of Rahul Malik?',\n",
    "    'When did Rahul Malik graduate from the university?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7e5f4-d415-4c1c-b31f-15c90d0e3673",
   "metadata": {},
   "source": [
    "Building response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f06e7eac-7717-441c-b8b7-d338b7e71728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What skills does Rahul Malik have?\n",
      "Answer: Python (NumPy, Pandas, Scikit-learn, Keras, Flask), SQL (MySQL, Postgres), Git, Time Series Forecasting, Productionizing Models, Recommendation Engines, Customer Segmentation, AWS, NLP \n",
      "---------------------------------------------\n",
      "Query: Where does Rahul Malik live?\n",
      "Answer: New York, NY  \n",
      "---------------------------------------------\n",
      "Query: What is the current position of Rahul Malik?\n",
      "Answer: NLP Data Scientist \n",
      "---------------------------------------------\n",
      "Query: What are the former employers of Rahul Malik?\n",
      "Answer: Priceline \n",
      "---------------------------------------------\n",
      "Query: When did Rahul Malik graduate from the university?\n",
      "Answer: April 2016 \n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    answer = rag_chain.invoke(query)\n",
    "    print(f\"Query: {query}\\nAnswer: {answer}{'-' * 45}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4fc61-2553-4d0b-aa18-f0490b4e8ddc",
   "metadata": {},
   "source": [
    "Comparing performance of RAG vs. pure LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4240741-71d5-4fa8-9a59-23cda0e5bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What skills does Rahul Malik have?\n",
      "Answer: Python (NumPy, Pandas, Scikit-learn, Keras, Flask), SQL (MySQL, Postgres), Git, Time Series Forecasting, Productionizing Models, Recommendation Engines, Customer Segmentation, AWS, NLP \n",
      "---------------------------------------------\n",
      "Query: Where does Rahul Malik live?\n",
      "Answer: Brooklyn, NY  \n",
      "---------------------------------------------\n",
      "Query: What is the current position of Rahul Malik?\n",
      "Answer: NLP Data Scientist  \n",
      "---------------------------------------------\n",
      "Query: What are the former employers of Rahul Malik?\n",
      "Answer: Amazon, Priceline, Microsoft \n",
      "---------------------------------------------\n",
      "Query: When did Rahul Malik graduate from the university?\n",
      "Answer: September 2010 - April 2016  \n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    answer = ollama.invoke(prompt.format(context = str(pages[0].page_content), query = query), temperature = 0.0)\n",
    "    print(f\"Query: {query}\\nAnswer: {answer}{'-' * 45}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634e018-6dd0-4fa9-b784-a8d7f95e8617",
   "metadata": {},
   "source": [
    "RAG and pure LLM returned quite similar answers. However, there are some differences that may indicate that RAG can be more precise than pure LLM. First of all, the fourth answer was more accurate for RAG. In the fourth question, I asked RAG about the former employers of the fictional character Rahul Malik from the CV. RAG returned one of the two names of past employers. In contrast, the LLM returned all organization names, including the name of Rahul Malik's current employer. In my opinion, this indicates that the LLM was not as precise.\r\n",
    "\r\n",
    "A similar situation occurred with the fifth query, where I asked for information about when the analyzed person graduated from university. RAG provided a precise answer, whereas the LLM failed in this task by not understanding the context. It returned the time span during which this fictional person studied instead of providing the correct information.\r\n",
    "\r\n",
    "These findings, along with an analysis of the theoretical aspects of both methods, allow me to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b5e85-5ace-43dc-87f5-165f9ad80196",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6c1fa-c319-4a86-8423-0974d1407b90",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16040d5-fb9a-4f9f-a47c-3011b128350e",
   "metadata": {},
   "source": [
    "How does RAG improve the quality and reliability of LLM responses compared to pure LLM generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316432f-b4d6-41ba-8dd1-a0dd2bcd2f12",
   "metadata": {},
   "source": [
    "RAG improves the quality and reliability of LLM responses compared to pure LLM generation by combining the generative power of LLMs with real-time information retrieval from external sources, such as databases. As a result, it can handle complex queries by pulling in relevant sections of retrieved text. Moreover, in contrast to LLMs, RAG mitigates hallucinations by grounding the model’s responses in actual retrieved documents or data. Additionally, RAG enhances the contextual relevance of responses. When the model retrieves specific documents or pieces of information, it can tailor its response more precisely to the user’s query. Finally, RAG can cross-reference multiple sources during generation, helping it maintain coherent and consistent facts in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7460068-82bc-4148-acd1-4ffb765b8175",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f380793-b4f5-4bfe-98ee-af3a863c4aec",
   "metadata": {},
   "source": [
    "What are the key factors affecting RAG performance (chunk size, embedding quality, prompt design)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24046ab6-49b8-40c4-b4f0-8db935fb692f",
   "metadata": {},
   "source": [
    "Factors such as: chunk size, embedding quality and prompt design play a crucial role in RAG performance. Chunk size affects the balance between precision and context. Smaller chunks improve retrieval accuracy, but may lose broader context, while larger chunks provide richer context, but risk including irrelevant details. The optimal size depends on the content and query type. Embedding quality ensures accurate retrieval by capturing semantic meaning. High-quality, domain-specific embeddings improve relevance, while poor embeddings lead to irrelevant or misleading retrievals. Additionally, well-structured prompt design guides the model in using retrieved information effectively, ensuring: logical, relevant and coherent responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89daf8-7e0c-41cd-88db-4f0b23299f53",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1075eb2-9bdc-457e-b987-adf42d87c843",
   "metadata": {},
   "source": [
    "How does the choice of vector database and embedding model impact system performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0df56-9e51-4d35-a58c-3da3529645fe",
   "metadata": {},
   "source": [
    "The choice of vector database and embedding model significantly impacts system performance. The vector database determines the efficiency of retrieving relevant information, which directly affects response time. This is achieved through features like efficient indexing and ANN search. Scalability is another critical factor, because a good vector database can handle large datasets while maintaining fast retrieval times. \r\n",
    "\r\n",
    "Similarly, the embedding model plays a crucial role in system performance. An efficient embedding model reduces latency and computational costs, while high-quality embeddings ensure better retrieval accuracy and more relevant results. Together, the vector database and embedding model form the foundation for a performant RAG system, influencing both speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f88f75-d272-4315-8e0d-a9ff1ad09d23",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6591c-1454-4764-a2f6-f66e165e34b1",
   "metadata": {},
   "source": [
    "What are the main challenges in implementing a production-ready RAG system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea13531-71e5-4d8b-a275-e94a4e39c22f",
   "metadata": {},
   "source": [
    "Implementing a production-ready Retrieval-Augmented Generation (RAG) system comes with several challenges. One of the main issues is the incompleteness of the knowledge base. This means that the data needed to answer a query may be unavailable or poorly formatted, making it difficult to use effectively. Another common challenge is ensuring the output is generated in the correct format. JSON is the standard format, but systems often produce incomplete or incorrectly structured outputs. Additionally, parsing complex documents, such as PDFs with embedded charts, tables or images, requires advanced parsers capable of handling these sophisticated structures. Finally, managing large-scale data retrieval presents significant challenges, as it demands efficient algorithms or specialized system architectures to ensure both accuracy and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b9c3dc-1568-4585-9be1-0ff047a56fb8",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0b1b1-a97d-47a0-b345-09e3440ffab0",
   "metadata": {},
   "source": [
    "How can the system be improved to handle complex queries requiring multiple document lookups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2a463-b5db-4907-be27-f72719edb0e6",
   "metadata": {},
   "source": [
    "The RAG system can be improved in various ways to handle complex queries requiring multiple document lookups. First, I recommend enabling multi-document retrieval and re-ranking, as these will ensure the most relevant information is prioritized. Another improvement worth considering is query decomposition and expansion, which can enhance understanding by breaking down complex queries and broadening the search scope. Advanced embedding models leveraging contextual and cross-document embeddings will enable better synthesis of information. Finally, post-processing techniques, such as information fusion and redundancy filtering, can further refine responses by combining insights and removing irrelevant data. These enhancements will allow the system to deliver accurate and relevant answers to complex multi-step queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
